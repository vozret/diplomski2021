Last login: Thu Apr  8 10:44:16 on ttys001
(base) Veronikas-MacBook-Pro:~ veronika$ ssh veronika@kamis-nn01.fesb.hr -p 10003
veronika@kamis-nn01.fesb.hr's password: 
Welcome to Ubuntu 20.04.1 LTS (GNU/Linux 5.4.0-65-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage
Last login: Thu Apr  8 08:45:13 2021 from 127.0.0.1
veronika@kamis-nn01:~$ cd radiIzvanka_QAApproved/
veronika@kamis-nn01:~/radiIzvanka_QAApproved$ cd probe/
veronika@kamis-nn01:~/radiIzvanka_QAApproved/probe$ vim proba4.py 
veronika@kamis-nn01:~/radiIzvanka_QAApproved/probe$ python3 proba4.py 
2021-04-08 08:53:07.504115: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-04-08 08:53:07.504159: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-04-08 08:53:10.374591: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-08 08:53:10.376031: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-04-08 08:53:10.421575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-04-08 08:53:10.422407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: 
pciBusID: 0000:81:00.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-04-08 08:53:10.422479: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-04-08 08:53:10.425776: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory
2021-04-08 08:53:10.425931: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory
2021-04-08 08:53:10.425977: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2021-04-08 08:53:10.426017: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2021-04-08 08:53:10.426062: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory
2021-04-08 08:53:10.426100: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory
2021-04-08 08:53:10.426137: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory
2021-04-08 08:53:10.426151: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2021-04-08 08:53:10.426503: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-04-08 08:53:10.426872: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-08 08:53:10.426897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-04-08 08:53:10.426905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
img (InputLayer)                [(None, 512, 512, 3) 0                                            
__________________________________________________________________________________________________
separable_conv2d (SeparableConv (None, 512, 512, 16) 91          img[0][0]                        
__________________________________________________________________________________________________
activation (Activation)         (None, 512, 512, 16) 0           separable_conv2d[0][0]           
__________________________________________________________________________________________________
separable_conv2d_1 (SeparableCo (None, 512, 512, 16) 416         activation[0][0]                 
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 512, 512, 16) 0           separable_conv2d_1[0][0]         
__________________________________________________________________________________________________
separable_conv2d_2 (SeparableCo (None, 512, 512, 16) 416         activation_1[0][0]               
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 512, 512, 16) 0           separable_conv2d_2[0][0]         
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 256, 256, 16) 0           activation_2[0][0]               
__________________________________________________________________________________________________
separable_conv2d_3 (SeparableCo (None, 256, 256, 32) 688         max_pooling2d[0][0]              
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 256, 256, 32) 0           separable_conv2d_3[0][0]         
__________________________________________________________________________________________________
separable_conv2d_4 (SeparableCo (None, 256, 256, 32) 1344        activation_3[0][0]               
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 256, 256, 32) 0           separable_conv2d_4[0][0]         
__________________________________________________________________________________________________
separable_conv2d_5 (SeparableCo (None, 256, 256, 32) 1344        activation_4[0][0]               
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 256, 256, 32) 0           separable_conv2d_5[0][0]         
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 32) 0           activation_5[0][0]               
__________________________________________________________________________________________________
separable_conv2d_6 (SeparableCo (None, 128, 128, 64) 2400        max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 128, 128, 64) 0           separable_conv2d_6[0][0]         
__________________________________________________________________________________________________
separable_conv2d_7 (SeparableCo (None, 128, 128, 64) 4736        activation_6[0][0]               
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 128, 128, 64) 0           separable_conv2d_7[0][0]         
__________________________________________________________________________________________________
separable_conv2d_8 (SeparableCo (None, 128, 128, 64) 4736        activation_7[0][0]               
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 128, 128, 64) 0           separable_conv2d_8[0][0]         
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 64)   0           activation_8[0][0]               
__________________________________________________________________________________________________
separable_conv2d_9 (SeparableCo (None, 64, 64, 128)  8896        max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 64, 64, 128)  0           separable_conv2d_9[0][0]         
__________________________________________________________________________________________________
separable_conv2d_10 (SeparableC (None, 64, 64, 128)  17664       activation_9[0][0]               
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 64, 64, 128)  0           separable_conv2d_10[0][0]        
__________________________________________________________________________________________________
separable_conv2d_11 (SeparableC (None, 64, 64, 128)  17664       activation_10[0][0]              
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 64, 64, 128)  0           separable_conv2d_11[0][0]        
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 128)  0           activation_11[0][0]              
__________________________________________________________________________________________________
separable_conv2d_12 (SeparableC (None, 32, 32, 256)  34176       max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 32, 32, 256)  0           separable_conv2d_12[0][0]        
__________________________________________________________________________________________________
separable_conv2d_13 (SeparableC (None, 32, 32, 256)  68096       activation_12[0][0]              
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 32, 32, 256)  0           separable_conv2d_13[0][0]        
__________________________________________________________________________________________________
separable_conv2d_14 (SeparableC (None, 32, 32, 256)  68096       activation_13[0][0]              
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 32, 32, 256)  0           separable_conv2d_14[0][0]        
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 16, 16, 256)  0           activation_14[0][0]              
__________________________________________________________________________________________________
separable_conv2d_15 (SeparableC (None, 16, 16, 512)  133888      max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 16, 16, 512)  0           separable_conv2d_15[0][0]        
__________________________________________________________________________________________________
separable_conv2d_16 (SeparableC (None, 16, 16, 512)  267264      activation_15[0][0]              
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 16, 16, 512)  0           separable_conv2d_16[0][0]        
__________________________________________________________________________________________________
separable_conv2d_17 (SeparableC (None, 16, 16, 512)  267264      activation_16[0][0]              
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 16, 16, 512)  0           separable_conv2d_17[0][0]        
__________________________________________________________________________________________________
conv2d_transpose (Conv2DTranspo (None, 32, 32, 256)  1179904     activation_17[0][0]              
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 32, 32, 512)  0           conv2d_transpose[0][0]           
                                                                 activation_14[0][0]              
__________________________________________________________________________________________________
separable_conv2d_18 (SeparableC (None, 32, 32, 128)  70272       concatenate[0][0]                
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 32, 32, 128)  0           separable_conv2d_18[0][0]        
__________________________________________________________________________________________________
separable_conv2d_19 (SeparableC (None, 32, 32, 128)  17664       activation_18[0][0]              
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 32, 32, 128)  0           separable_conv2d_19[0][0]        
__________________________________________________________________________________________________
separable_conv2d_20 (SeparableC (None, 32, 32, 128)  17664       activation_19[0][0]              
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 32, 32, 128)  0           separable_conv2d_20[0][0]        
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 64, 64, 128)  147584      activation_20[0][0]              
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 64, 64, 256)  0           conv2d_transpose_1[0][0]         
                                                                 activation_11[0][0]              
__________________________________________________________________________________________________
separable_conv2d_21 (SeparableC (None, 64, 64, 128)  35200       concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 64, 64, 128)  0           separable_conv2d_21[0][0]        
__________________________________________________________________________________________________
separable_conv2d_22 (SeparableC (None, 64, 64, 128)  17664       activation_21[0][0]              
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 64, 64, 128)  0           separable_conv2d_22[0][0]        
__________________________________________________________________________________________________
separable_conv2d_23 (SeparableC (None, 64, 64, 128)  17664       activation_22[0][0]              
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 64, 64, 128)  0           separable_conv2d_23[0][0]        
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 128, 128, 64) 73792       activation_23[0][0]              
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 128, 128, 128 0           conv2d_transpose_2[0][0]         
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
separable_conv2d_24 (SeparableC (None, 128, 128, 64) 9408        concatenate_2[0][0]              
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 128, 128, 64) 0           separable_conv2d_24[0][0]        
__________________________________________________________________________________________________
separable_conv2d_25 (SeparableC (None, 128, 128, 64) 4736        activation_24[0][0]              
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 128, 128, 64) 0           separable_conv2d_25[0][0]        
__________________________________________________________________________________________________
separable_conv2d_26 (SeparableC (None, 128, 128, 64) 4736        activation_25[0][0]              
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 128, 128, 64) 0           separable_conv2d_26[0][0]        
__________________________________________________________________________________________________
conv2d_transpose_3 (Conv2DTrans (None, 256, 256, 32) 18464       activation_26[0][0]              
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 256, 256, 64) 0           conv2d_transpose_3[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
separable_conv2d_27 (SeparableC (None, 256, 256, 32) 2656        concatenate_3[0][0]              
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 256, 256, 32) 0           separable_conv2d_27[0][0]        
__________________________________________________________________________________________________
separable_conv2d_28 (SeparableC (None, 256, 256, 32) 1344        activation_27[0][0]              
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 256, 256, 32) 0           separable_conv2d_28[0][0]        
__________________________________________________________________________________________________
separable_conv2d_29 (SeparableC (None, 256, 256, 32) 1344        activation_28[0][0]              
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 256, 256, 32) 0           separable_conv2d_29[0][0]        
__________________________________________________________________________________________________
conv2d_transpose_4 (Conv2DTrans (None, 512, 512, 16) 4624        activation_29[0][0]              
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 512, 512, 32) 0           conv2d_transpose_4[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
separable_conv2d_30 (SeparableC (None, 512, 512, 16) 816         concatenate_4[0][0]              
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 512, 512, 16) 0           separable_conv2d_30[0][0]        
__________________________________________________________________________________________________
separable_conv2d_31 (SeparableC (None, 512, 512, 16) 416         activation_30[0][0]              
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 512, 512, 16) 0           separable_conv2d_31[0][0]        
__________________________________________________________________________________________________
separable_conv2d_32 (SeparableC (None, 512, 512, 16) 416         activation_31[0][0]              
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 512, 512, 16) 0           separable_conv2d_32[0][0]        
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 512, 512, 3)  51          activation_32[0][0]              
==================================================================================================
Total params: 2,525,598
Trainable params: 2,525,598
Non-trainable params: 0
__________________________________________________________________________________________________
None
2021-04-08 08:53:11.639921: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-04-08 08:53:11.668605: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2999955000 Hz
Epoch 1/1000
36/36 [==============================] - 312s 9s/step - loss: 0.6697 - accuracy: 0.0128 - val_loss: 0.6410 - val_accuracy: 0.2483

Epoch 00001: val_loss improved from inf to 0.64100, saving model to model-proba4-3.h5
Epoch 2/1000
36/36 [==============================] - 313s 9s/step - loss: 0.6582 - accuracy: 0.3062 - val_loss: 0.6387 - val_accuracy: 0.3562

Epoch 00002: val_loss improved from 0.64100 to 0.63866, saving model to model-proba4-3.h5
Epoch 3/1000
36/36 [==============================] - 308s 9s/step - loss: 0.6536 - accuracy: 0.2850 - val_loss: 0.6303 - val_accuracy: 0.1770

Epoch 00003: val_loss improved from 0.63866 to 0.63031, saving model to model-proba4-3.h5
Epoch 4/1000
36/36 [==============================] - 313s 9s/step - loss: 0.6436 - accuracy: 0.1863 - val_loss: 0.6313 - val_accuracy: 0.1151

Epoch 00004: val_loss did not improve from 0.63031
Epoch 5/1000
36/36 [==============================] - 314s 9s/step - loss: 0.6430 - accuracy: 0.1820 - val_loss: 0.6298 - val_accuracy: 0.0883

Epoch 00005: val_loss improved from 0.63031 to 0.62979, saving model to model-proba4-3.h5
Epoch 6/1000
36/36 [==============================] - 315s 9s/step - loss: 0.6389 - accuracy: 0.2083 - val_loss: 0.6282 - val_accuracy: 0.4083

Epoch 00006: val_loss improved from 0.62979 to 0.62818, saving model to model-proba4-3.h5
Epoch 7/1000
36/36 [==============================] - 316s 9s/step - loss: 0.6369 - accuracy: 0.2423 - val_loss: 0.6271 - val_accuracy: 0.0946

Epoch 00007: val_loss improved from 0.62818 to 0.62705, saving model to model-proba4-3.h5
Epoch 8/1000
36/36 [==============================] - 324s 9s/step - loss: 0.6402 - accuracy: 0.1550 - val_loss: 0.6346 - val_accuracy: 0.4268

Epoch 00008: val_loss did not improve from 0.62705
Epoch 9/1000
36/36 [==============================] - 315s 9s/step - loss: 0.6349 - accuracy: 0.4084 - val_loss: 0.6285 - val_accuracy: 0.1955

Epoch 00009: val_loss did not improve from 0.62705
Epoch 10/1000
36/36 [==============================] - 316s 9s/step - loss: 0.6333 - accuracy: 0.1616 - val_loss: 0.6262 - val_accuracy: 0.2771

Epoch 00010: val_loss improved from 0.62705 to 0.62617, saving model to model-proba4-3.h5
Epoch 11/1000
36/36 [==============================] - 306s 9s/step - loss: 0.6253 - accuracy: 0.2106 - val_loss: 0.6260 - val_accuracy: 0.3577

Epoch 00011: val_loss improved from 0.62617 to 0.62596, saving model to model-proba4-3.h5
Epoch 12/1000
36/36 [==============================] - 319s 9s/step - loss: 0.6300 - accuracy: 0.3158 - val_loss: 0.6338 - val_accuracy: 0.1745

Epoch 00012: val_loss did not improve from 0.62596
Epoch 13/1000
36/36 [==============================] - 305s 8s/step - loss: 0.6317 - accuracy: 0.2235 - val_loss: 0.6283 - val_accuracy: 0.3033

Epoch 00013: val_loss did not improve from 0.62596
Epoch 14/1000
36/36 [==============================] - 326s 9s/step - loss: 0.6311 - accuracy: 0.3453 - val_loss: 0.6290 - val_accuracy: 0.5253

Epoch 00014: val_loss did not improve from 0.62596
Epoch 15/1000
36/36 [==============================] - 317s 9s/step - loss: 0.6215 - accuracy: 0.5523 - val_loss: 0.6313 - val_accuracy: 0.3244

Epoch 00015: val_loss did not improve from 0.62596
Epoch 16/1000
36/36 [==============================] - 320s 9s/step - loss: 0.6328 - accuracy: 0.4209 - val_loss: 0.6306 - val_accuracy: 0.2249

Epoch 00016: val_loss did not improve from 0.62596
Epoch 17/1000
36/36 [==============================] - 315s 9s/step - loss: 0.6309 - accuracy: 0.3223 - val_loss: 0.6331 - val_accuracy: 0.4988

Epoch 00017: val_loss did not improve from 0.62596
Epoch 18/1000
36/36 [==============================] - 323s 9s/step - loss: 0.6276 - accuracy: 0.4081 - val_loss: 0.6264 - val_accuracy: 0.4711

Epoch 00018: val_loss did not improve from 0.62596
Epoch 19/1000
36/36 [==============================] - 315s 9s/step - loss: 0.6297 - accuracy: 0.4185 - val_loss: 0.6266 - val_accuracy: 0.4555

Epoch 00019: val_loss did not improve from 0.62596
Epoch 20/1000
36/36 [==============================] - 310s 9s/step - loss: 0.6297 - accuracy: 0.3342 - val_loss: 0.6259 - val_accuracy: 0.5093

Epoch 00020: val_loss improved from 0.62596 to 0.62589, saving model to model-proba4-3.h5
Epoch 21/1000
36/36 [==============================] - 323s 9s/step - loss: 0.6272 - accuracy: 0.4678 - val_loss: 0.6288 - val_accuracy: 0.1977

Epoch 00021: val_loss did not improve from 0.62589
Epoch 22/1000
36/36 [==============================] - 311s 9s/step - loss: 0.6273 - accuracy: 0.2512 - val_loss: 0.6299 - val_accuracy: 0.2488

Epoch 00022: val_loss did not improve from 0.62589
Epoch 23/1000
36/36 [==============================] - 324s 9s/step - loss: 0.6279 - accuracy: 0.3709 - val_loss: 0.6280 - val_accuracy: 0.4778

Epoch 00023: val_loss did not improve from 0.62589
Epoch 24/1000
36/36 [==============================] - 323s 9s/step - loss: 0.6277 - accuracy: 0.3832 - val_loss: 0.6381 - val_accuracy: 0.1140

Epoch 00024: val_loss did not improve from 0.62589
Epoch 25/1000
36/36 [==============================] - 325s 9s/step - loss: 0.6255 - accuracy: 0.2125 - val_loss: 0.6249 - val_accuracy: 0.2978

Epoch 00025: val_loss improved from 0.62589 to 0.62491, saving model to model-proba4-3.h5
Epoch 26/1000
36/36 [==============================] - 331s 9s/step - loss: 0.6289 - accuracy: 0.3340 - val_loss: 0.6268 - val_accuracy: 0.3169

Epoch 00026: val_loss did not improve from 0.62491
Epoch 27/1000
36/36 [==============================] - 316s 9s/step - loss: 0.6232 - accuracy: 0.2858 - val_loss: 0.6231 - val_accuracy: 0.6193

Epoch 00027: val_loss improved from 0.62491 to 0.62306, saving model to model-proba4-3.h5
Epoch 28/1000
36/36 [==============================] - 326s 9s/step - loss: 0.6249 - accuracy: 0.4401 - val_loss: 0.6284 - val_accuracy: 0.3524

Epoch 00028: val_loss did not improve from 0.62306
Epoch 29/1000
36/36 [==============================] - 318s 9s/step - loss: 0.6255 - accuracy: 0.3146 - val_loss: 0.6340 - val_accuracy: 0.2353

Epoch 00029: val_loss did not improve from 0.62306
Epoch 30/1000
36/36 [==============================] - 322s 9s/step - loss: 0.6245 - accuracy: 0.2637 - val_loss: 0.6266 - val_accuracy: 0.6149

Epoch 00030: val_loss did not improve from 0.62306
Epoch 31/1000
36/36 [==============================] - 327s 9s/step - loss: 0.6255 - accuracy: 0.3362 - val_loss: 0.6258 - val_accuracy: 0.4129

Epoch 00031: val_loss did not improve from 0.62306
Epoch 32/1000
36/36 [==============================] - 322s 9s/step - loss: 0.6256 - accuracy: 0.3717 - val_loss: 0.6387 - val_accuracy: 0.2189

Epoch 00032: val_loss did not improve from 0.62306
Epoch 33/1000
36/36 [==============================] - 319s 9s/step - loss: 0.6267 - accuracy: 0.3434 - val_loss: 0.6282 - val_accuracy: 0.1952

Epoch 00033: val_loss did not improve from 0.62306
Epoch 34/1000
36/36 [==============================] - 308s 9s/step - loss: 0.6267 - accuracy: 0.2826 - val_loss: 0.6254 - val_accuracy: 0.3750

Epoch 00034: val_loss did not improve from 0.62306
Epoch 35/1000
36/36 [==============================] - 313s 9s/step - loss: 0.6216 - accuracy: 0.4061 - val_loss: 0.6283 - val_accuracy: 0.4570

Epoch 00035: val_loss did not improve from 0.62306
Epoch 36/1000
36/36 [==============================] - 327s 9s/step - loss: 0.6260 - accuracy: 0.4243 - val_loss: 0.6277 - val_accuracy: 0.3708

Epoch 00036: val_loss did not improve from 0.62306
Epoch 37/1000
36/36 [==============================] - 313s 9s/step - loss: 0.6223 - accuracy: 0.4402 - val_loss: 0.6282 - val_accuracy: 0.3441

Epoch 00037: val_loss did not improve from 0.62306
Epoch 38/1000
36/36 [==============================] - 326s 9s/step - loss: 0.6217 - accuracy: 0.4004 - val_loss: 0.6406 - val_accuracy: 0.4322

Epoch 00038: val_loss did not improve from 0.62306
Epoch 39/1000
36/36 [==============================] - 316s 9s/step - loss: 0.6249 - accuracy: 0.4353 - val_loss: 0.6262 - val_accuracy: 0.3529

Epoch 00039: val_loss did not improve from 0.62306
Epoch 40/1000
36/36 [==============================] - 305s 8s/step - loss: 0.6185 - accuracy: 0.4204 - val_loss: 0.6347 - val_accuracy: 0.5988

Epoch 00040: val_loss did not improve from 0.62306
Epoch 41/1000
36/36 [==============================] - 325s 9s/step - loss: 0.6238 - accuracy: 0.4616 - val_loss: 0.6291 - val_accuracy: 0.5728

Epoch 00041: val_loss did not improve from 0.62306
Epoch 42/1000
36/36 [==============================] - 324s 9s/step - loss: 0.6192 - accuracy: 0.5062 - val_loss: 0.6327 - val_accuracy: 0.3419

Epoch 00042: val_loss did not improve from 0.62306
Epoch 43/1000
36/36 [==============================] - 323s 9s/step - loss: 0.6246 - accuracy: 0.3665 - val_loss: 0.6269 - val_accuracy: 0.4123

Epoch 00043: val_loss did not improve from 0.62306
Epoch 44/1000
36/36 [==============================] - 315s 9s/step - loss: 0.6194 - accuracy: 0.4128 - val_loss: 0.6269 - val_accuracy: 0.3322

Epoch 00044: val_loss did not improve from 0.62306
Epoch 45/1000
36/36 [==============================] - 323s 9s/step - loss: 0.6193 - accuracy: 0.3865 - val_loss: 0.6241 - val_accuracy: 0.3040

Epoch 00045: val_loss did not improve from 0.62306
Epoch 46/1000
36/36 [==============================] - 324s 9s/step - loss: 0.6172 - accuracy: 0.3595 - val_loss: 0.6280 - val_accuracy: 0.3339

Epoch 00046: val_loss did not improve from 0.62306
Epoch 47/1000
36/36 [==============================] - 329s 9s/step - loss: 0.6229 - accuracy: 0.3134 - val_loss: 0.6294 - val_accuracy: 0.4098

Epoch 00047: val_loss did not improve from 0.62306
Epoch 48/1000
36/36 [==============================] - 340s 9s/step - loss: 0.6184 - accuracy: 0.3704 - val_loss: 0.6271 - val_accuracy: 0.2555

Epoch 00048: val_loss did not improve from 0.62306
Epoch 49/1000
36/36 [==============================] - 320s 9s/step - loss: 0.6256 - accuracy: 0.3579 - val_loss: 0.6254 - val_accuracy: 0.3537

Epoch 00049: val_loss did not improve from 0.62306
Epoch 50/1000
36/36 [==============================] - 314s 9s/step - loss: 0.6173 - accuracy: 0.3968 - val_loss: 0.6254 - val_accuracy: 0.4217

Epoch 00050: val_loss did not improve from 0.62306
Epoch 51/1000
36/36 [==============================] - 324s 9s/step - loss: 0.6155 - accuracy: 0.3550 - val_loss: 0.6199 - val_accuracy: 0.1717

Epoch 00051: val_loss improved from 0.62306 to 0.61989, saving model to model-proba4-3.h5
Epoch 52/1000
36/36 [==============================] - 324s 9s/step - loss: 0.6157 - accuracy: 0.3132 - val_loss: 0.6269 - val_accuracy: 0.3852

Epoch 00052: val_loss did not improve from 0.61989
Epoch 53/1000
36/36 [==============================] - 315s 9s/step - loss: 0.6152 - accuracy: 0.3527 - val_loss: 0.6342 - val_accuracy: 0.4486

Epoch 00053: val_loss did not improve from 0.61989
Epoch 54/1000
36/36 [==============================] - 323s 9s/step - loss: 0.6234 - accuracy: 0.3338 - val_loss: 0.6245 - val_accuracy: 0.2787

Epoch 00054: val_loss did not improve from 0.61989
Epoch 55/1000
36/36 [==============================] - 316s 9s/step - loss: 0.6193 - accuracy: 0.4023 - val_loss: 0.6234 - val_accuracy: 0.5392

Epoch 00055: val_loss did not improve from 0.61989
Epoch 56/1000
36/36 [==============================] - 334s 9s/step - loss: 0.6171 - accuracy: 0.4043 - val_loss: 0.6252 - val_accuracy: 0.5955

Epoch 00056: val_loss did not improve from 0.61989
Epoch 57/1000
36/36 [==============================] - 318s 9s/step - loss: 0.6167 - accuracy: 0.4457 - val_loss: 0.6215 - val_accuracy: 0.4272

Epoch 00057: val_loss did not improve from 0.61989
Epoch 58/1000
36/36 [==============================] - 332s 9s/step - loss: 0.6184 - accuracy: 0.3413 - val_loss: 0.6297 - val_accuracy: 0.2683

Epoch 00058: val_loss did not improve from 0.61989
Epoch 59/1000
36/36 [==============================] - 313s 9s/step - loss: 0.6138 - accuracy: 0.3760 - val_loss: 0.6300 - val_accuracy: 0.4811

Epoch 00059: val_loss did not improve from 0.61989
Epoch 60/1000
36/36 [==============================] - 317s 9s/step - loss: 0.6176 - accuracy: 0.3802 - val_loss: 0.6193 - val_accuracy: 0.4382

Epoch 00060: val_loss improved from 0.61989 to 0.61928, saving model to model-proba4-3.h5
Epoch 61/1000
36/36 [==============================] - 318s 9s/step - loss: 0.6148 - accuracy: 0.3791 - val_loss: 0.6229 - val_accuracy: 0.4804

Epoch 00061: val_loss did not improve from 0.61928
Epoch 62/1000
36/36 [==============================] - 317s 9s/step - loss: 0.6168 - accuracy: 0.3801 - val_loss: 0.6434 - val_accuracy: 0.2678

Epoch 00062: val_loss did not improve from 0.61928
Epoch 63/1000
36/36 [==============================] - 327s 9s/step - loss: 0.6140 - accuracy: 0.3128 - val_loss: 0.6189 - val_accuracy: 0.3819

Epoch 00063: val_loss improved from 0.61928 to 0.61892, saving model to model-proba4-3.h5
Epoch 64/1000
36/36 [==============================] - 335s 9s/step - loss: 0.6131 - accuracy: 0.3197 - val_loss: 0.6197 - val_accuracy: 0.2776

Epoch 00064: val_loss did not improve from 0.61892
Epoch 65/1000
36/36 [==============================] - 326s 9s/step - loss: 0.6151 - accuracy: 0.3300 - val_loss: 0.6265 - val_accuracy: 0.2046

Epoch 00065: val_loss did not improve from 0.61892
Epoch 66/1000
36/36 [==============================] - 315s 9s/step - loss: 0.6184 - accuracy: 0.3257 - val_loss: 0.6278 - val_accuracy: 0.1789

Epoch 00066: val_loss did not improve from 0.61892
Epoch 67/1000
36/36 [==============================] - 307s 9s/step - loss: 0.6133 - accuracy: 0.2942 - val_loss: 0.6363 - val_accuracy: 0.3856

Epoch 00067: val_loss did not improve from 0.61892
Epoch 68/1000
36/36 [==============================] - 328s 9s/step - loss: 0.6143 - accuracy: 0.3419 - val_loss: 0.6167 - val_accuracy: 0.2597

Epoch 00068: val_loss improved from 0.61892 to 0.61673, saving model to model-proba4-3.h5
Epoch 69/1000
36/36 [==============================] - 310s 9s/step - loss: 0.6138 - accuracy: 0.3508 - val_loss: 0.6345 - val_accuracy: 0.4527

Epoch 00069: val_loss did not improve from 0.61673
Epoch 70/1000
36/36 [==============================] - 318s 9s/step - loss: 0.6131 - accuracy: 0.3666 - val_loss: 0.6204 - val_accuracy: 0.3938

Epoch 00070: val_loss did not improve from 0.61673
Epoch 71/1000
36/36 [==============================] - 318s 9s/step - loss: 0.6162 - accuracy: 0.3607 - val_loss: 0.6288 - val_accuracy: 0.3335

Epoch 00071: val_loss did not improve from 0.61673
Epoch 72/1000
36/36 [==============================] - 313s 9s/step - loss: 0.6157 - accuracy: 0.3409 - val_loss: 0.6162 - val_accuracy: 0.3801

Epoch 00072: val_loss improved from 0.61673 to 0.61621, saving model to model-proba4-3.h5
Epoch 73/1000
36/36 [==============================] - 323s 9s/step - loss: 0.6178 - accuracy: 0.3908 - val_loss: 0.6264 - val_accuracy: 0.3956

Epoch 00073: val_loss did not improve from 0.61621
Epoch 74/1000
36/36 [==============================] - 319s 9s/step - loss: 0.6045 - accuracy: 0.3498 - val_loss: 0.6267 - val_accuracy: 0.2827

Epoch 00074: val_loss did not improve from 0.61621
Epoch 75/1000
36/36 [==============================] - 335s 9s/step - loss: 0.6120 - accuracy: 0.3471 - val_loss: 0.6248 - val_accuracy: 0.4062

Epoch 00075: val_loss did not improve from 0.61621
Epoch 76/1000
36/36 [==============================] - 329s 9s/step - loss: 0.6135 - accuracy: 0.3053 - val_loss: 0.6245 - val_accuracy: 0.3845

Epoch 00076: val_loss did not improve from 0.61621
Epoch 77/1000
36/36 [==============================] - 313s 9s/step - loss: 0.6117 - accuracy: 0.3506 - val_loss: 0.6189 - val_accuracy: 0.4559

Epoch 00077: val_loss did not improve from 0.61621
Epoch 78/1000
36/36 [==============================] - 329s 9s/step - loss: 0.6150 - accuracy: 0.3989 - val_loss: 0.6136 - val_accuracy: 0.1911

Epoch 00078: val_loss improved from 0.61621 to 0.61362, saving model to model-proba4-3.h5
Epoch 79/1000
36/36 [==============================] - 320s 9s/step - loss: 0.6152 - accuracy: 0.3522 - val_loss: 0.6197 - val_accuracy: 0.3690

Epoch 00079: val_loss did not improve from 0.61362
Epoch 80/1000
36/36 [==============================] - 331s 9s/step - loss: 0.6063 - accuracy: 0.3625 - val_loss: 0.6231 - val_accuracy: 0.4258

Epoch 00080: val_loss did not improve from 0.61362
Epoch 81/1000
36/36 [==============================] - 311s 9s/step - loss: 0.6114 - accuracy: 0.3898 - val_loss: 0.6156 - val_accuracy: 0.2531

Epoch 00081: val_loss did not improve from 0.61362
Epoch 82/1000
36/36 [==============================] - 320s 9s/step - loss: 0.6078 - accuracy: 0.3487 - val_loss: 0.6161 - val_accuracy: 0.2294

Epoch 00082: val_loss did not improve from 0.61362
Epoch 83/1000
36/36 [==============================] - 329s 9s/step - loss: 0.6147 - accuracy: 0.3349 - val_loss: 0.6158 - val_accuracy: 0.4350

Epoch 00083: val_loss did not improve from 0.61362
Epoch 84/1000
36/36 [==============================] - 319s 9s/step - loss: 0.6120 - accuracy: 0.3579 - val_loss: 0.6264 - val_accuracy: 0.3504

Epoch 00084: val_loss did not improve from 0.61362
Epoch 85/1000
36/36 [==============================] - 327s 9s/step - loss: 0.6088 - accuracy: 0.3633 - val_loss: 0.6192 - val_accuracy: 0.2746

Epoch 00085: val_loss did not improve from 0.61362
Epoch 86/1000
36/36 [==============================] - 318s 9s/step - loss: 0.6113 - accuracy: 0.3289 - val_loss: 0.6246 - val_accuracy: 0.4432

Epoch 00086: val_loss did not improve from 0.61362
Epoch 87/1000
36/36 [==============================] - 318s 9s/step - loss: 0.6113 - accuracy: 0.3930 - val_loss: 0.6307 - val_accuracy: 0.3546

Epoch 00087: val_loss did not improve from 0.61362
Epoch 88/1000
36/36 [==============================] - 325s 9s/step - loss: 0.6108 - accuracy: 0.3185 - val_loss: 0.6235 - val_accuracy: 0.3455

Epoch 00088: val_loss did not improve from 0.61362
Epoch 89/1000
36/36 [==============================] - 312s 9s/step - loss: 0.6112 - accuracy: 0.2849 - val_loss: 0.6225 - val_accuracy: 0.3310

Epoch 00089: val_loss did not improve from 0.61362
Epoch 90/1000
36/36 [==============================] - 332s 9s/step - loss: 0.6095 - accuracy: 0.3672 - val_loss: 0.6298 - val_accuracy: 0.2274

Epoch 00090: val_loss did not improve from 0.61362
Epoch 91/1000
36/36 [==============================] - 318s 9s/step - loss: 0.6082 - accuracy: 0.3212 - val_loss: 0.6235 - val_accuracy: 0.3374

Epoch 00091: val_loss did not improve from 0.61362
Epoch 92/1000
36/36 [==============================] - 324s 9s/step - loss: 0.6085 - accuracy: 0.3245 - val_loss: 0.6239 - val_accuracy: 0.5046

Epoch 00092: val_loss did not improve from 0.61362
Epoch 93/1000
36/36 [==============================] - 322s 9s/step - loss: 0.6110 - accuracy: 0.3776 - val_loss: 0.6159 - val_accuracy: 0.3078

Epoch 00093: val_loss did not improve from 0.61362
Epoch 94/1000
36/36 [==============================] - 309s 9s/step - loss: 0.6098 - accuracy: 0.3082 - val_loss: 0.6099 - val_accuracy: 0.3179

Epoch 00094: val_loss improved from 0.61362 to 0.60990, saving model to model-proba4-3.h5
Epoch 95/1000
36/36 [==============================] - 319s 9s/step - loss: 0.6067 - accuracy: 0.3074 - val_loss: 0.6216 - val_accuracy: 0.2361

Epoch 00095: val_loss did not improve from 0.60990
Epoch 96/1000
36/36 [==============================] - 333s 9s/step - loss: 0.6076 - accuracy: 0.2943 - val_loss: 0.6284 - val_accuracy: 0.2830

Epoch 00096: val_loss did not improve from 0.60990
Epoch 97/1000
36/36 [==============================] - 327s 9s/step - loss: 0.6096 - accuracy: 0.3035 - val_loss: 0.6115 - val_accuracy: 0.2901

Epoch 00097: val_loss did not improve from 0.60990
Epoch 98/1000
36/36 [==============================] - 331s 9s/step - loss: 0.6060 - accuracy: 0.2937 - val_loss: 0.6331 - val_accuracy: 0.3012

Epoch 00098: val_loss did not improve from 0.60990
Epoch 99/1000
36/36 [==============================] - 325s 9s/step - loss: 0.6090 - accuracy: 0.3049 - val_loss: 0.6117 - val_accuracy: 0.2602

Epoch 00099: val_loss did not improve from 0.60990
Epoch 100/1000
36/36 [==============================] - 320s 9s/step - loss: 0.6104 - accuracy: 0.3167 - val_loss: 0.6461 - val_accuracy: 0.3444

Epoch 00100: val_loss did not improve from 0.60990
Epoch 101/1000
36/36 [==============================] - 329s 9s/step - loss: 0.6155 - accuracy: 0.3099 - val_loss: 0.6255 - val_accuracy: 0.3848

Epoch 00101: val_loss did not improve from 0.60990
Epoch 102/1000
36/36 [==============================] - 308s 9s/step - loss: 0.6142 - accuracy: 0.3445 - val_loss: 0.6205 - val_accuracy: 0.4078

Epoch 00102: val_loss did not improve from 0.60990
Epoch 103/1000
36/36 [==============================] - 336s 9s/step - loss: 0.6077 - accuracy: 0.3393 - val_loss: 0.6174 - val_accuracy: 0.3375

Epoch 00103: val_loss did not improve from 0.60990
Epoch 104/1000
36/36 [==============================] - 319s 9s/step - loss: 0.6096 - accuracy: 0.3383 - val_loss: 0.6277 - val_accuracy: 0.3439

Epoch 00104: val_loss did not improve from 0.60990
Epoch 105/1000
36/36 [==============================] - 337s 9s/step - loss: 0.6063 - accuracy: 0.3031 - val_loss: 0.6153 - val_accuracy: 0.2771

Epoch 00105: val_loss did not improve from 0.60990
Epoch 106/1000
36/36 [==============================] - 324s 9s/step - loss: 0.6060 - accuracy: 0.2963 - val_loss: 0.6170 - val_accuracy: 0.3378

Epoch 00106: val_loss did not improve from 0.60990
Epoch 107/1000
36/36 [==============================] - 305s 8s/step - loss: 0.6039 - accuracy: 0.3253 - val_loss: 0.6124 - val_accuracy: 0.3621

Epoch 00107: val_loss did not improve from 0.60990
Epoch 108/1000
36/36 [==============================] - 320s 9s/step - loss: 0.6060 - accuracy: 0.3217 - val_loss: 0.6080 - val_accuracy: 0.2573

Epoch 00108: val_loss improved from 0.60990 to 0.60804, saving model to model-proba4-3.h5
Epoch 109/1000
36/36 [==============================] - 319s 9s/step - loss: 0.6095 - accuracy: 0.3099 - val_loss: 0.6119 - val_accuracy: 0.3233

Epoch 00109: val_loss did not improve from 0.60804
Epoch 110/1000
36/36 [==============================] - 326s 9s/step - loss: 0.6089 - accuracy: 0.3199 - val_loss: 0.6192 - val_accuracy: 0.3039

Epoch 00110: val_loss did not improve from 0.60804
Epoch 111/1000
36/36 [==============================] - 325s 9s/step - loss: 0.6072 - accuracy: 0.3182 - val_loss: 0.6271 - val_accuracy: 0.3221

Epoch 00111: val_loss did not improve from 0.60804
Epoch 112/1000
36/36 [==============================] - 346s 10s/step - loss: 0.6030 - accuracy: 0.3082 - val_loss: 0.6188 - val_accuracy: 0.3484

Epoch 00112: val_loss did not improve from 0.60804
Epoch 113/1000
36/36 [==============================] - 330s 9s/step - loss: 0.6045 - accuracy: 0.3123 - val_loss: 0.6252 - val_accuracy: 0.3449

Epoch 00113: val_loss did not improve from 0.60804
Epoch 114/1000
36/36 [==============================] - 324s 9s/step - loss: 0.6125 - accuracy: 0.3196 - val_loss: 0.6147 - val_accuracy: 0.3092

Epoch 00114: val_loss did not improve from 0.60804
Epoch 115/1000
36/36 [==============================] - 326s 9s/step - loss: 0.6109 - accuracy: 0.2932 - val_loss: 0.6203 - val_accuracy: 0.2868

Epoch 00115: val_loss did not improve from 0.60804
Epoch 116/1000
36/36 [==============================] - 329s 9s/step - loss: 0.6037 - accuracy: 0.2910 - val_loss: 0.6231 - val_accuracy: 0.3431

Epoch 00116: val_loss did not improve from 0.60804
Epoch 117/1000
36/36 [==============================] - 322s 9s/step - loss: 0.6096 - accuracy: 0.3162 - val_loss: 0.6241 - val_accuracy: 0.2989

Epoch 00117: val_loss did not improve from 0.60804
Epoch 118/1000
36/36 [==============================] - 329s 9s/step - loss: 0.6056 - accuracy: 0.2846 - val_loss: 0.6151 - val_accuracy: 0.3717

Epoch 00118: val_loss did not improve from 0.60804
Epoch 119/1000
36/36 [==============================] - 322s 9s/step - loss: 0.6018 - accuracy: 0.3129 - val_loss: 0.6057 - val_accuracy: 0.2774

Epoch 00119: val_loss improved from 0.60804 to 0.60568, saving model to model-proba4-3.h5
Epoch 120/1000
36/36 [==============================] - 319s 9s/step - loss: 0.6081 - accuracy: 0.3056 - val_loss: 0.6145 - val_accuracy: 0.4004

Epoch 00120: val_loss did not improve from 0.60568
Epoch 121/1000
36/36 [==============================] - 334s 9s/step - loss: 0.6044 - accuracy: 0.3773 - val_loss: 0.6111 - val_accuracy: 0.2951

Epoch 00121: val_loss did not improve from 0.60568
Epoch 122/1000
36/36 [==============================] - 328s 9s/step - loss: 0.6013 - accuracy: 0.2833 - val_loss: 0.6087 - val_accuracy: 0.3459

Epoch 00122: val_loss did not improve from 0.60568
Epoch 123/1000
36/36 [==============================] - 326s 9s/step - loss: 0.6071 - accuracy: 0.3106 - val_loss: 0.6118 - val_accuracy: 0.2860

Epoch 00123: val_loss did not improve from 0.60568
Epoch 124/1000
36/36 [==============================] - 319s 9s/step - loss: 0.6008 - accuracy: 0.2973 - val_loss: 0.6184 - val_accuracy: 0.2700

Epoch 00124: val_loss did not improve from 0.60568
Epoch 125/1000
36/36 [==============================] - 335s 9s/step - loss: 0.6097 - accuracy: 0.2991 - val_loss: 0.6062 - val_accuracy: 0.2909

Epoch 00125: val_loss did not improve from 0.60568
Epoch 126/1000
36/36 [==============================] - 319s 9s/step - loss: 0.6023 - accuracy: 0.3004 - val_loss: 0.6081 - val_accuracy: 0.3480

Epoch 00126: val_loss did not improve from 0.60568
Epoch 127/1000
36/36 [==============================] - 315s 9s/step - loss: 0.6045 - accuracy: 0.3132 - val_loss: 0.6171 - val_accuracy: 0.3507

Epoch 00127: val_loss did not improve from 0.60568
Epoch 128/1000
36/36 [==============================] - 327s 9s/step - loss: 0.6076 - accuracy: 0.3026 - val_loss: 0.6148 - val_accuracy: 0.2781

Epoch 00128: val_loss did not improve from 0.60568
Epoch 129/1000
36/36 [==============================] - 338s 9s/step - loss: 0.6032 - accuracy: 0.3146 - val_loss: 0.6036 - val_accuracy: 0.3244

Epoch 00129: val_loss improved from 0.60568 to 0.60359, saving model to model-proba4-3.h5
Epoch 130/1000
36/36 [==============================] - 325s 9s/step - loss: 0.6068 - accuracy: 0.3125 - val_loss: 0.6034 - val_accuracy: 0.2917

Epoch 00130: val_loss improved from 0.60359 to 0.60345, saving model to model-proba4-3.h5
Epoch 131/1000
36/36 [==============================] - 322s 9s/step - loss: 0.6057 - accuracy: 0.3147 - val_loss: 0.6106 - val_accuracy: 0.3924

Epoch 00131: val_loss did not improve from 0.60345
Epoch 132/1000
36/36 [==============================] - 322s 9s/step - loss: 0.6038 - accuracy: 0.3462 - val_loss: 0.6028 - val_accuracy: 0.3139

Epoch 00132: val_loss improved from 0.60345 to 0.60284, saving model to model-proba4-3.h5
Epoch 133/1000
36/36 [==============================] - 329s 9s/step - loss: 0.5987 - accuracy: 0.2916 - val_loss: 0.6133 - val_accuracy: 0.3271

Epoch 00133: val_loss did not improve from 0.60284
Epoch 134/1000
36/36 [==============================] - 331s 9s/step - loss: 0.5978 - accuracy: 0.2950 - val_loss: 0.6189 - val_accuracy: 0.3113

Epoch 00134: val_loss did not improve from 0.60284
Epoch 135/1000
36/36 [==============================] - 319s 9s/step - loss: 0.6006 - accuracy: 0.2966 - val_loss: 0.6067 - val_accuracy: 0.3679

Epoch 00135: val_loss did not improve from 0.60284
Epoch 136/1000
36/36 [==============================] - 329s 9s/step - loss: 0.6033 - accuracy: 0.3115 - val_loss: 0.6176 - val_accuracy: 0.2857

Epoch 00136: val_loss did not improve from 0.60284
Epoch 137/1000
36/36 [==============================] - 325s 9s/step - loss: 0.6026 - accuracy: 0.3041 - val_loss: 0.6104 - val_accuracy: 0.4326

Epoch 00137: val_loss did not improve from 0.60284
Epoch 138/1000
36/36 [==============================] - 328s 9s/step - loss: 0.6037 - accuracy: 0.3254 - val_loss: 0.6251 - val_accuracy: 0.2832

Epoch 00138: val_loss did not improve from 0.60284
Epoch 139/1000
36/36 [==============================] - 316s 9s/step - loss: 0.6048 - accuracy: 0.2862 - val_loss: 0.6194 - val_accuracy: 0.2791

Epoch 00139: val_loss did not improve from 0.60284
Epoch 140/1000
36/36 [==============================] - 320s 9s/step - loss: 0.5998 - accuracy: 0.2837 - val_loss: 0.6147 - val_accuracy: 0.3281

Epoch 00140: val_loss did not improve from 0.60284
Epoch 141/1000
36/36 [==============================] - 324s 9s/step - loss: 0.6031 - accuracy: 0.3015 - val_loss: 0.6066 - val_accuracy: 0.3486

Epoch 00141: val_loss did not improve from 0.60284
Epoch 142/1000
36/36 [==============================] - 337s 9s/step - loss: 0.6058 - accuracy: 0.2938 - val_loss: 0.6055 - val_accuracy: 0.3056

Epoch 00142: val_loss did not improve from 0.60284
Epoch 143/1000
36/36 [==============================] - 315s 9s/step - loss: 0.6029 - accuracy: 0.3030 - val_loss: 0.6250 - val_accuracy: 0.2536

Epoch 00143: val_loss did not improve from 0.60284
Epoch 144/1000
36/36 [==============================] - 330s 9s/step - loss: 0.6026 - accuracy: 0.2806 - val_loss: 0.6090 - val_accuracy: 0.4128

Epoch 00144: val_loss did not improve from 0.60284
Epoch 145/1000
36/36 [==============================] - 343s 9s/step - loss: 0.6010 - accuracy: 0.3260 - val_loss: 0.6102 - val_accuracy: 0.4642

Epoch 00145: val_loss did not improve from 0.60284
Epoch 146/1000
36/36 [==============================] - 330s 9s/step - loss: 0.5995 - accuracy: 0.3289 - val_loss: 0.6252 - val_accuracy: 0.2983

Epoch 00146: val_loss did not improve from 0.60284
Epoch 147/1000
36/36 [==============================] - 332s 9s/step - loss: 0.6062 - accuracy: 0.2810 - val_loss: 0.6188 - val_accuracy: 0.2735

Epoch 00147: val_loss did not improve from 0.60284
Epoch 148/1000
36/36 [==============================] - 328s 9s/step - loss: 0.5983 - accuracy: 0.2838 - val_loss: 0.6100 - val_accuracy: 0.3514

Epoch 00148: val_loss did not improve from 0.60284
Epoch 149/1000
36/36 [==============================] - 339s 9s/step - loss: 0.5993 - accuracy: 0.2795 - val_loss: 0.6199 - val_accuracy: 0.2441

Epoch 00149: val_loss did not improve from 0.60284
Epoch 150/1000
36/36 [==============================] - 334s 9s/step - loss: 0.6032 - accuracy: 0.2757 - val_loss: 0.6106 - val_accuracy: 0.2674

Epoch 00150: val_loss did not improve from 0.60284
Epoch 151/1000
36/36 [==============================] - 325s 9s/step - loss: 0.5983 - accuracy: 0.3146 - val_loss: 0.6055 - val_accuracy: 0.3008

Epoch 00151: val_loss did not improve from 0.60284
Epoch 152/1000
36/36 [==============================] - 325s 9s/step - loss: 0.5949 - accuracy: 0.3068 - val_loss: 0.6210 - val_accuracy: 0.3014

Epoch 00152: val_loss did not improve from 0.60284
Epoch 153/1000
36/36 [==============================] - 322s 9s/step - loss: 0.6022 - accuracy: 0.3015 - val_loss: 0.6231 - val_accuracy: 0.3666

Epoch 00153: val_loss did not improve from 0.60284
Epoch 154/1000
36/36 [==============================] - 334s 9s/step - loss: 0.5999 - accuracy: 0.2969 - val_loss: 0.6047 - val_accuracy: 0.3257

Epoch 00154: val_loss did not improve from 0.60284
Epoch 155/1000
36/36 [==============================] - 315s 9s/step - loss: 0.5983 - accuracy: 0.3077 - val_loss: 0.6034 - val_accuracy: 0.3087

Epoch 00155: val_loss did not improve from 0.60284
Epoch 156/1000
36/36 [==============================] - 335s 9s/step - loss: 0.6039 - accuracy: 0.2869 - val_loss: 0.6083 - val_accuracy: 0.2136

Epoch 00156: val_loss did not improve from 0.60284
Epoch 157/1000
36/36 [==============================] - 334s 9s/step - loss: 0.5998 - accuracy: 0.2849 - val_loss: 0.5996 - val_accuracy: 0.3072

Epoch 00157: val_loss improved from 0.60284 to 0.59958, saving model to model-proba4-3.h5
Epoch 158/1000
36/36 [==============================] - 335s 9s/step - loss: 0.6022 - accuracy: 0.2972 - val_loss: 0.6054 - val_accuracy: 0.3615

Epoch 00158: val_loss did not improve from 0.59958
Epoch 159/1000
36/36 [==============================] - 324s 9s/step - loss: 0.5997 - accuracy: 0.3070 - val_loss: 0.5995 - val_accuracy: 0.3759

Epoch 00159: val_loss improved from 0.59958 to 0.59946, saving model to model-proba4-3.h5
Epoch 160/1000
36/36 [==============================] - 328s 9s/step - loss: 0.5987 - accuracy: 0.3124 - val_loss: 0.6012 - val_accuracy: 0.3101

Epoch 00160: val_loss did not improve from 0.59946
Epoch 161/1000
36/36 [==============================] - 321s 9s/step - loss: 0.5969 - accuracy: 0.3096 - val_loss: 0.6110 - val_accuracy: 0.2518

Epoch 00161: val_loss did not improve from 0.59946
Epoch 162/1000
36/36 [==============================] - 329s 9s/step - loss: 0.5986 - accuracy: 0.2731 - val_loss: 0.6036 - val_accuracy: 0.2668

Epoch 00162: val_loss did not improve from 0.59946
Epoch 163/1000
36/36 [==============================] - 323s 9s/step - loss: 0.6045 - accuracy: 0.2820 - val_loss: 0.6148 - val_accuracy: 0.1833

Epoch 00163: val_loss did not improve from 0.59946
Epoch 164/1000
36/36 [==============================] - 325s 9s/step - loss: 0.6047 - accuracy: 0.2859 - val_loss: 0.5995 - val_accuracy: 0.2729

Epoch 00164: val_loss did not improve from 0.59946
Epoch 165/1000
36/36 [==============================] - 327s 9s/step - loss: 0.5991 - accuracy: 0.2821 - val_loss: 0.6078 - val_accuracy: 0.3345

Epoch 00165: val_loss did not improve from 0.59946
Epoch 166/1000
18/36 [==============>...............] - ETA: 2:45 - loss: 0.6026 - accuracy: 0.2599packet_write_wait: Connection to 161.53.171.105 port 10003: Broken pipe
(base) Veronikas-MacBook-Pro:~ veronika$ ssh veronika@kamis-nn01.fesb.hr -p 10003
veronika@kamis-nn01.fesb.hr's password: 
Welcome to Ubuntu 20.04.1 LTS (GNU/Linux 5.4.0-65-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage
Last login: Thu Apr  8 08:50:02 2021 from 127.0.0.1
veronika@kamis-nn01:~$ vim proba4.py 
veronika@kamis-nn01:~$ cd radiIzvanka_QAApproved/
veronika@kamis-nn01:~/radiIzvanka_QAApproved$ cd probe/
veronika@kamis-nn01:~/radiIzvanka_QAApproved/probe$ vim proba4.py 
veronika@kamis-nn01:~/radiIzvanka_QAApproved/probe$ python3 proba4.py 
2021-04-09 07:10:52.460613: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-04-09 07:10:52.460649: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-04-09 07:10:55.429158: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-09 07:10:55.430590: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-04-09 07:10:55.466730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-04-09 07:10:55.467523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: 
pciBusID: 0000:81:00.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-04-09 07:10:55.467579: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-04-09 07:10:55.468165: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory
2021-04-09 07:10:55.468287: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory
2021-04-09 07:10:55.468326: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2021-04-09 07:10:55.468366: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2021-04-09 07:10:55.468398: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory
2021-04-09 07:10:55.468431: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory
2021-04-09 07:10:55.468464: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory
2021-04-09 07:10:55.468473: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2021-04-09 07:10:55.468673: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-04-09 07:10:55.471148: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-09 07:10:55.471224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-04-09 07:10:55.471234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
img (InputLayer)                [(None, 512, 512, 3) 0                                            
__________________________________________________________________________________________________
separable_conv2d (SeparableConv (None, 512, 512, 16) 91          img[0][0]                        
__________________________________________________________________________________________________
activation (Activation)         (None, 512, 512, 16) 0           separable_conv2d[0][0]           
__________________________________________________________________________________________________
separable_conv2d_1 (SeparableCo (None, 512, 512, 16) 416         activation[0][0]                 
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 512, 512, 16) 0           separable_conv2d_1[0][0]         
__________________________________________________________________________________________________
separable_conv2d_2 (SeparableCo (None, 512, 512, 16) 416         activation_1[0][0]               
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 512, 512, 16) 0           separable_conv2d_2[0][0]         
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 256, 256, 16) 0           activation_2[0][0]               
__________________________________________________________________________________________________
separable_conv2d_3 (SeparableCo (None, 256, 256, 32) 688         max_pooling2d[0][0]              
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 256, 256, 32) 0           separable_conv2d_3[0][0]         
__________________________________________________________________________________________________
separable_conv2d_4 (SeparableCo (None, 256, 256, 32) 1344        activation_3[0][0]               
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 256, 256, 32) 0           separable_conv2d_4[0][0]         
__________________________________________________________________________________________________
separable_conv2d_5 (SeparableCo (None, 256, 256, 32) 1344        activation_4[0][0]               
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 256, 256, 32) 0           separable_conv2d_5[0][0]         
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 32) 0           activation_5[0][0]               
__________________________________________________________________________________________________
separable_conv2d_6 (SeparableCo (None, 128, 128, 64) 2400        max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 128, 128, 64) 0           separable_conv2d_6[0][0]         
__________________________________________________________________________________________________
separable_conv2d_7 (SeparableCo (None, 128, 128, 64) 4736        activation_6[0][0]               
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 128, 128, 64) 0           separable_conv2d_7[0][0]         
__________________________________________________________________________________________________
separable_conv2d_8 (SeparableCo (None, 128, 128, 64) 4736        activation_7[0][0]               
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 128, 128, 64) 0           separable_conv2d_8[0][0]         
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 64)   0           activation_8[0][0]               
__________________________________________________________________________________________________
separable_conv2d_9 (SeparableCo (None, 64, 64, 128)  8896        max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 64, 64, 128)  0           separable_conv2d_9[0][0]         
__________________________________________________________________________________________________
separable_conv2d_10 (SeparableC (None, 64, 64, 128)  17664       activation_9[0][0]               
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 64, 64, 128)  0           separable_conv2d_10[0][0]        
__________________________________________________________________________________________________
separable_conv2d_11 (SeparableC (None, 64, 64, 128)  17664       activation_10[0][0]              
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 64, 64, 128)  0           separable_conv2d_11[0][0]        
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 128)  0           activation_11[0][0]              
__________________________________________________________________________________________________
separable_conv2d_12 (SeparableC (None, 32, 32, 256)  34176       max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 32, 32, 256)  0           separable_conv2d_12[0][0]        
__________________________________________________________________________________________________
separable_conv2d_13 (SeparableC (None, 32, 32, 256)  68096       activation_12[0][0]              
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 32, 32, 256)  0           separable_conv2d_13[0][0]        
__________________________________________________________________________________________________
separable_conv2d_14 (SeparableC (None, 32, 32, 256)  68096       activation_13[0][0]              
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 32, 32, 256)  0           separable_conv2d_14[0][0]        
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 16, 16, 256)  0           activation_14[0][0]              
__________________________________________________________________________________________________
separable_conv2d_15 (SeparableC (None, 16, 16, 512)  133888      max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 16, 16, 512)  0           separable_conv2d_15[0][0]        
__________________________________________________________________________________________________
separable_conv2d_16 (SeparableC (None, 16, 16, 512)  267264      activation_15[0][0]              
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 16, 16, 512)  0           separable_conv2d_16[0][0]        
__________________________________________________________________________________________________
separable_conv2d_17 (SeparableC (None, 16, 16, 512)  267264      activation_16[0][0]              
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 16, 16, 512)  0           separable_conv2d_17[0][0]        
__________________________________________________________________________________________________
conv2d_transpose (Conv2DTranspo (None, 32, 32, 256)  1179904     activation_17[0][0]              
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 32, 32, 512)  0           conv2d_transpose[0][0]           
                                                                 activation_14[0][0]              
__________________________________________________________________________________________________
separable_conv2d_18 (SeparableC (None, 32, 32, 128)  70272       concatenate[0][0]                
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 32, 32, 128)  0           separable_conv2d_18[0][0]        
__________________________________________________________________________________________________
separable_conv2d_19 (SeparableC (None, 32, 32, 128)  17664       activation_18[0][0]              
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 32, 32, 128)  0           separable_conv2d_19[0][0]        
__________________________________________________________________________________________________
separable_conv2d_20 (SeparableC (None, 32, 32, 128)  17664       activation_19[0][0]              
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 32, 32, 128)  0           separable_conv2d_20[0][0]        
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 64, 64, 128)  147584      activation_20[0][0]              
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 64, 64, 256)  0           conv2d_transpose_1[0][0]         
                                                                 activation_11[0][0]              
__________________________________________________________________________________________________
separable_conv2d_21 (SeparableC (None, 64, 64, 128)  35200       concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 64, 64, 128)  0           separable_conv2d_21[0][0]        
__________________________________________________________________________________________________
separable_conv2d_22 (SeparableC (None, 64, 64, 128)  17664       activation_21[0][0]              
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 64, 64, 128)  0           separable_conv2d_22[0][0]        
__________________________________________________________________________________________________
separable_conv2d_23 (SeparableC (None, 64, 64, 128)  17664       activation_22[0][0]              
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 64, 64, 128)  0           separable_conv2d_23[0][0]        
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 128, 128, 64) 73792       activation_23[0][0]              
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 128, 128, 128 0           conv2d_transpose_2[0][0]         
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
separable_conv2d_24 (SeparableC (None, 128, 128, 64) 9408        concatenate_2[0][0]              
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 128, 128, 64) 0           separable_conv2d_24[0][0]        
__________________________________________________________________________________________________
separable_conv2d_25 (SeparableC (None, 128, 128, 64) 4736        activation_24[0][0]              
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 128, 128, 64) 0           separable_conv2d_25[0][0]        
__________________________________________________________________________________________________
separable_conv2d_26 (SeparableC (None, 128, 128, 64) 4736        activation_25[0][0]              
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 128, 128, 64) 0           separable_conv2d_26[0][0]        
__________________________________________________________________________________________________
conv2d_transpose_3 (Conv2DTrans (None, 256, 256, 32) 18464       activation_26[0][0]              
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 256, 256, 64) 0           conv2d_transpose_3[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
separable_conv2d_27 (SeparableC (None, 256, 256, 32) 2656        concatenate_3[0][0]              
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 256, 256, 32) 0           separable_conv2d_27[0][0]        
__________________________________________________________________________________________________
separable_conv2d_28 (SeparableC (None, 256, 256, 32) 1344        activation_27[0][0]              
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 256, 256, 32) 0           separable_conv2d_28[0][0]        
__________________________________________________________________________________________________
separable_conv2d_29 (SeparableC (None, 256, 256, 32) 1344        activation_28[0][0]              
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 256, 256, 32) 0           separable_conv2d_29[0][0]        
__________________________________________________________________________________________________
conv2d_transpose_4 (Conv2DTrans (None, 512, 512, 16) 4624        activation_29[0][0]              
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 512, 512, 32) 0           conv2d_transpose_4[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
separable_conv2d_30 (SeparableC (None, 512, 512, 16) 816         concatenate_4[0][0]              
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 512, 512, 16) 0           separable_conv2d_30[0][0]        
__________________________________________________________________________________________________
separable_conv2d_31 (SeparableC (None, 512, 512, 16) 416         activation_30[0][0]              
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 512, 512, 16) 0           separable_conv2d_31[0][0]        
__________________________________________________________________________________________________
separable_conv2d_32 (SeparableC (None, 512, 512, 16) 416         activation_31[0][0]              
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 512, 512, 16) 0           separable_conv2d_32[0][0]        
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 512, 512, 3)  51          activation_32[0][0]              
==================================================================================================
Total params: 2,525,598
Trainable params: 2,525,598
Non-trainable params: 0
__________________________________________________________________________________________________
None
2021-04-09 07:10:56.720220: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-04-09 07:10:56.745123: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2999955000 Hz
Epoch 1/1000
36/36 [==============================] - 345s 9s/step - loss: 0.6736 - accuracy: 0.0014 - val_loss: 0.6421 - val_accuracy: 0.1079

Epoch 00001: val_loss improved from inf to 0.64212, saving model to model-proba4-4.h5
Epoch 2/1000
36/36 [==============================] - 325s 9s/step - loss: 0.6550 - accuracy: 0.2158 - val_loss: 0.6351 - val_accuracy: 0.4462

Epoch 00002: val_loss improved from 0.64212 to 0.63513, saving model to model-proba4-4.h5
Epoch 3/1000
36/36 [==============================] - 337s 9s/step - loss: 0.6539 - accuracy: 0.3590 - val_loss: 0.6302 - val_accuracy: 0.5367

Epoch 00003: val_loss improved from 0.63513 to 0.63017, saving model to model-proba4-4.h5
Epoch 4/1000
36/36 [==============================] - 326s 9s/step - loss: 0.6389 - accuracy: 0.5166 - val_loss: 0.6262 - val_accuracy: 0.4573

Epoch 00004: val_loss improved from 0.63017 to 0.62625, saving model to model-proba4-4.h5
Epoch 5/1000
36/36 [==============================] - 316s 9s/step - loss: 0.6446 - accuracy: 0.4433 - val_loss: 0.6220 - val_accuracy: 0.2437

Epoch 00005: val_loss improved from 0.62625 to 0.62200, saving model to model-proba4-4.h5
Epoch 6/1000
36/36 [==============================] - 334s 9s/step - loss: 0.6346 - accuracy: 0.1669 - val_loss: 0.6247 - val_accuracy: 0.1988

Epoch 00006: val_loss did not improve from 0.62200
Epoch 7/1000
36/36 [==============================] - 327s 9s/step - loss: 0.6337 - accuracy: 0.1853 - val_loss: 0.6278 - val_accuracy: 0.6887

Epoch 00007: val_loss did not improve from 0.62200
Epoch 8/1000
36/36 [==============================] - 318s 9s/step - loss: 0.6347 - accuracy: 0.4423 - val_loss: 0.6210 - val_accuracy: 0.0793

Epoch 00008: val_loss improved from 0.62200 to 0.62102, saving model to model-proba4-4.h5
Epoch 9/1000
36/36 [==============================] - 336s 9s/step - loss: 0.6292 - accuracy: 0.1470 - val_loss: 0.6214 - val_accuracy: 0.0850

Epoch 00009: val_loss did not improve from 0.62102
Epoch 10/1000
36/36 [==============================] - 341s 9s/step - loss: 0.6384 - accuracy: 0.2570 - val_loss: 0.6206 - val_accuracy: 0.1072

Epoch 00010: val_loss improved from 0.62102 to 0.62062, saving model to model-proba4-4.h5
Epoch 11/1000
36/36 [==============================] - 329s 9s/step - loss: 0.6285 - accuracy: 0.2957 - val_loss: 0.6305 - val_accuracy: 0.1162

Epoch 00011: val_loss did not improve from 0.62062
Epoch 12/1000
36/36 [==============================] - 320s 9s/step - loss: 0.6298 - accuracy: 0.2560 - val_loss: 0.6248 - val_accuracy: 0.0961

Epoch 00012: val_loss did not improve from 0.62062
Epoch 13/1000
36/36 [==============================] - 339s 9s/step - loss: 0.6312 - accuracy: 0.1401 - val_loss: 0.6250 - val_accuracy: 0.2296

Epoch 00013: val_loss did not improve from 0.62062
Epoch 14/1000
36/36 [==============================] - 306s 8s/step - loss: 0.6279 - accuracy: 0.3024 - val_loss: 0.6286 - val_accuracy: 0.1823

Epoch 00014: val_loss did not improve from 0.62062
Epoch 15/1000
36/36 [==============================] - 326s 9s/step - loss: 0.6320 - accuracy: 0.1549 - val_loss: 0.6259 - val_accuracy: 0.0969

Epoch 00015: val_loss did not improve from 0.62062
Epoch 16/1000
36/36 [==============================] - 326s 9s/step - loss: 0.6299 - accuracy: 0.2585 - val_loss: 0.6196 - val_accuracy: 0.1398

Epoch 00016: val_loss improved from 0.62062 to 0.61961, saving model to model-proba4-4.h5
Epoch 17/1000
36/36 [==============================] - 315s 9s/step - loss: 0.6226 - accuracy: 0.1743 - val_loss: 0.6285 - val_accuracy: 0.1220

Epoch 00017: val_loss did not improve from 0.61961
Epoch 18/1000
36/36 [==============================] - 322s 9s/step - loss: 0.6334 - accuracy: 0.2793 - val_loss: 0.6239 - val_accuracy: 0.0854

Epoch 00018: val_loss did not improve from 0.61961
Epoch 19/1000
36/36 [==============================] - 317s 9s/step - loss: 0.6289 - accuracy: 0.1360 - val_loss: 0.6380 - val_accuracy: 0.1904

Epoch 00019: val_loss did not improve from 0.61961
Epoch 20/1000
36/36 [==============================] - 324s 9s/step - loss: 0.6289 - accuracy: 0.2422 - val_loss: 0.6213 - val_accuracy: 0.3410

Epoch 00020: val_loss did not improve from 0.61961
Epoch 21/1000
36/36 [==============================] - 316s 9s/step - loss: 0.6256 - accuracy: 0.2550 - val_loss: 0.6314 - val_accuracy: 0.1294

Epoch 00021: val_loss did not improve from 0.61961
Epoch 22/1000
36/36 [==============================] - 324s 9s/step - loss: 0.6254 - accuracy: 0.2415 - val_loss: 0.6249 - val_accuracy: 0.1476

Epoch 00022: val_loss did not improve from 0.61961
Epoch 23/1000
36/36 [==============================] - 314s 9s/step - loss: 0.6250 - accuracy: 0.2314 - val_loss: 0.6259 - val_accuracy: 0.4043

Epoch 00023: val_loss did not improve from 0.61961
Epoch 24/1000
36/36 [==============================] - 327s 9s/step - loss: 0.6267 - accuracy: 0.3914 - val_loss: 0.6298 - val_accuracy: 0.4275

Epoch 00024: val_loss did not improve from 0.61961
Epoch 25/1000
36/36 [==============================] - 344s 9s/step - loss: 0.6270 - accuracy: 0.3525 - val_loss: 0.6188 - val_accuracy: 0.1950

Epoch 00025: val_loss improved from 0.61961 to 0.61876, saving model to model-proba4-4.h5
Epoch 26/1000
36/36 [==============================] - 320s 9s/step - loss: 0.6257 - accuracy: 0.2511 - val_loss: 0.6299 - val_accuracy: 0.2196

Epoch 00026: val_loss did not improve from 0.61876
Epoch 27/1000
36/36 [==============================] - 314s 9s/step - loss: 0.6253 - accuracy: 0.2520 - val_loss: 0.6149 - val_accuracy: 0.2117

Epoch 00027: val_loss improved from 0.61876 to 0.61493, saving model to model-proba4-4.h5
Epoch 28/1000
36/36 [==============================] - 329s 9s/step - loss: 0.6276 - accuracy: 0.3251 - val_loss: 0.6266 - val_accuracy: 0.1501

Epoch 00028: val_loss did not improve from 0.61493
Epoch 29/1000
36/36 [==============================] - 332s 9s/step - loss: 0.6258 - accuracy: 0.2110 - val_loss: 0.6193 - val_accuracy: 0.3892

Epoch 00029: val_loss did not improve from 0.61493
Epoch 30/1000
36/36 [==============================] - 324s 9s/step - loss: 0.6252 - accuracy: 0.3675 - val_loss: 0.6185 - val_accuracy: 0.4752

Epoch 00030: val_loss did not improve from 0.61493
Epoch 31/1000
36/36 [==============================] - 319s 9s/step - loss: 0.6193 - accuracy: 0.4221 - val_loss: 0.6321 - val_accuracy: 0.3562

Epoch 00031: val_loss did not improve from 0.61493
Epoch 32/1000
36/36 [==============================] - 336s 9s/step - loss: 0.6272 - accuracy: 0.3396 - val_loss: 0.6158 - val_accuracy: 0.2708

Epoch 00032: val_loss did not improve from 0.61493
Epoch 33/1000
36/36 [==============================] - 324s 9s/step - loss: 0.6245 - accuracy: 0.3086 - val_loss: 0.6311 - val_accuracy: 0.1804

Epoch 00033: val_loss did not improve from 0.61493
Epoch 34/1000
36/36 [==============================] - 327s 9s/step - loss: 0.6236 - accuracy: 0.2469 - val_loss: 0.6243 - val_accuracy: 0.4327

Epoch 00034: val_loss did not improve from 0.61493
Epoch 35/1000
36/36 [==============================] - 324s 9s/step - loss: 0.6273 - accuracy: 0.4155 - val_loss: 0.6209 - val_accuracy: 0.1180

Epoch 00035: val_loss did not improve from 0.61493
Epoch 36/1000
36/36 [==============================] - 329s 9s/step - loss: 0.6276 - accuracy: 0.2597 - val_loss: 0.6160 - val_accuracy: 0.3042

Epoch 00036: val_loss did not improve from 0.61493
Epoch 37/1000
36/36 [==============================] - 313s 9s/step - loss: 0.6268 - accuracy: 0.3056 - val_loss: 0.6262 - val_accuracy: 0.4008

Epoch 00037: val_loss did not improve from 0.61493
Epoch 38/1000
36/36 [==============================] - 325s 9s/step - loss: 0.6210 - accuracy: 0.2617 - val_loss: 0.6208 - val_accuracy: 0.2551

Epoch 00038: val_loss did not improve from 0.61493
Epoch 39/1000
36/36 [==============================] - 326s 9s/step - loss: 0.6243 - accuracy: 0.3145 - val_loss: 0.6279 - val_accuracy: 0.4180

Epoch 00039: val_loss did not improve from 0.61493
Epoch 40/1000
36/36 [==============================] - 328s 9s/step - loss: 0.6224 - accuracy: 0.3398 - val_loss: 0.6211 - val_accuracy: 0.2027

Epoch 00040: val_loss did not improve from 0.61493
Epoch 41/1000
36/36 [==============================] - 327s 9s/step - loss: 0.6210 - accuracy: 0.2891 - val_loss: 0.6239 - val_accuracy: 0.1400

Epoch 00041: val_loss did not improve from 0.61493
Epoch 42/1000
36/36 [==============================] - 329s 9s/step - loss: 0.6228 - accuracy: 0.3036 - val_loss: 0.6225 - val_accuracy: 0.5002

Epoch 00042: val_loss did not improve from 0.61493
Epoch 43/1000
36/36 [==============================] - 320s 9s/step - loss: 0.6245 - accuracy: 0.4206 - val_loss: 0.6265 - val_accuracy: 0.5125

Epoch 00043: val_loss did not improve from 0.61493
Epoch 44/1000
36/36 [==============================] - 328s 9s/step - loss: 0.6184 - accuracy: 0.3709 - val_loss: 0.6251 - val_accuracy: 0.2287

Epoch 00044: val_loss did not improve from 0.61493
Epoch 45/1000
36/36 [==============================] - 322s 9s/step - loss: 0.6143 - accuracy: 0.2988 - val_loss: 0.6203 - val_accuracy: 0.3111

Epoch 00045: val_loss did not improve from 0.61493
Epoch 46/1000
36/36 [==============================] - 322s 9s/step - loss: 0.6215 - accuracy: 0.3203 - val_loss: 0.6123 - val_accuracy: 0.3051

Epoch 00046: val_loss improved from 0.61493 to 0.61229, saving model to model-proba4-4.h5
Epoch 47/1000
36/36 [==============================] - 328s 9s/step - loss: 0.6196 - accuracy: 0.3807 - val_loss: 0.6198 - val_accuracy: 0.1737

Epoch 00047: val_loss did not improve from 0.61229
Epoch 48/1000
36/36 [==============================] - 323s 9s/step - loss: 0.6159 - accuracy: 0.3131 - val_loss: 0.6426 - val_accuracy: 0.6018

Epoch 00048: val_loss did not improve from 0.61229
Epoch 49/1000
36/36 [==============================] - 325s 9s/step - loss: 0.6179 - accuracy: 0.4422 - val_loss: 0.6274 - val_accuracy: 0.6235

Epoch 00049: val_loss did not improve from 0.61229
Epoch 50/1000
36/36 [==============================] - 323s 9s/step - loss: 0.6209 - accuracy: 0.4336 - val_loss: 0.6156 - val_accuracy: 0.2503

Epoch 00050: val_loss did not improve from 0.61229
Epoch 51/1000
36/36 [==============================] - 324s 9s/step - loss: 0.6200 - accuracy: 0.3374 - val_loss: 0.6256 - val_accuracy: 0.4585

Epoch 00051: val_loss did not improve from 0.61229
Epoch 52/1000
36/36 [==============================] - 315s 9s/step - loss: 0.6172 - accuracy: 0.4422 - val_loss: 0.6165 - val_accuracy: 0.6430

Epoch 00052: val_loss did not improve from 0.61229
Epoch 53/1000
36/36 [==============================] - 318s 9s/step - loss: 0.6147 - accuracy: 0.4510 - val_loss: 0.6194 - val_accuracy: 0.4097

Epoch 00053: val_loss did not improve from 0.61229
Epoch 54/1000
36/36 [==============================] - 321s 9s/step - loss: 0.6186 - accuracy: 0.3864 - val_loss: 0.6143 - val_accuracy: 0.3237

Epoch 00054: val_loss did not improve from 0.61229
Epoch 55/1000
36/36 [==============================] - 324s 9s/step - loss: 0.6199 - accuracy: 0.4471 - val_loss: 0.6197 - val_accuracy: 0.5320

Epoch 00055: val_loss did not improve from 0.61229
Epoch 56/1000
36/36 [==============================] - 327s 9s/step - loss: 0.6224 - accuracy: 0.4359 - val_loss: 0.6111 - val_accuracy: 0.1475

Epoch 00056: val_loss improved from 0.61229 to 0.61112, saving model to model-proba4-4.h5
Epoch 57/1000
36/36 [==============================] - 332s 9s/step - loss: 0.6213 - accuracy: 0.3958 - val_loss: 0.6183 - val_accuracy: 0.4690

Epoch 00057: val_loss did not improve from 0.61112
Epoch 58/1000
36/36 [==============================] - 325s 9s/step - loss: 0.6185 - accuracy: 0.4814 - val_loss: 0.6336 - val_accuracy: 0.6523

Epoch 00058: val_loss did not improve from 0.61112
Epoch 59/1000
36/36 [==============================] - 327s 9s/step - loss: 0.6111 - accuracy: 0.3924 - val_loss: 0.6191 - val_accuracy: 0.3512

Epoch 00059: val_loss did not improve from 0.61112
Epoch 60/1000
36/36 [==============================] - 314s 9s/step - loss: 0.6198 - accuracy: 0.4266 - val_loss: 0.6412 - val_accuracy: 0.6304

Epoch 00060: val_loss did not improve from 0.61112
Epoch 61/1000
36/36 [==============================] - 328s 9s/step - loss: 0.6188 - accuracy: 0.4288 - val_loss: 0.6301 - val_accuracy: 0.5040

Epoch 00061: val_loss did not improve from 0.61112
Epoch 62/1000
36/36 [==============================] - 322s 9s/step - loss: 0.6163 - accuracy: 0.4011 - val_loss: 0.6220 - val_accuracy: 0.5567

Epoch 00062: val_loss did not improve from 0.61112
Epoch 63/1000
36/36 [==============================] - 326s 9s/step - loss: 0.6169 - accuracy: 0.4944 - val_loss: 0.6168 - val_accuracy: 0.4814

Epoch 00063: val_loss did not improve from 0.61112
Epoch 64/1000
36/36 [==============================] - 323s 9s/step - loss: 0.6125 - accuracy: 0.3945 - val_loss: 0.6190 - val_accuracy: 0.3553

Epoch 00064: val_loss did not improve from 0.61112
Epoch 65/1000
36/36 [==============================] - 328s 9s/step - loss: 0.6105 - accuracy: 0.3703 - val_loss: 0.6261 - val_accuracy: 0.3292

Epoch 00065: val_loss did not improve from 0.61112
Epoch 66/1000
36/36 [==============================] - 323s 9s/step - loss: 0.6129 - accuracy: 0.3508 - val_loss: 0.6167 - val_accuracy: 0.5058

Epoch 00066: val_loss did not improve from 0.61112
Epoch 67/1000
36/36 [==============================] - 326s 9s/step - loss: 0.6147 - accuracy: 0.4594 - val_loss: 0.6168 - val_accuracy: 0.4718

Epoch 00067: val_loss did not improve from 0.61112
Epoch 68/1000
36/36 [==============================] - 330s 9s/step - loss: 0.6132 - accuracy: 0.4726 - val_loss: 0.6223 - val_accuracy: 0.2697

Epoch 00068: val_loss did not improve from 0.61112
Epoch 69/1000
36/36 [==============================] - 323s 9s/step - loss: 0.6115 - accuracy: 0.3151 - val_loss: 0.6244 - val_accuracy: 0.4816

Epoch 00069: val_loss did not improve from 0.61112
Epoch 70/1000
36/36 [==============================] - 323s 9s/step - loss: 0.6110 - accuracy: 0.4420 - val_loss: 0.6148 - val_accuracy: 0.3870

Epoch 00070: val_loss did not improve from 0.61112
Epoch 71/1000
36/36 [==============================] - 319s 9s/step - loss: 0.6122 - accuracy: 0.3914 - val_loss: 0.6388 - val_accuracy: 0.3690

Epoch 00071: val_loss did not improve from 0.61112
Epoch 72/1000
36/36 [==============================] - 327s 9s/step - loss: 0.6066 - accuracy: 0.3321 - val_loss: 0.6305 - val_accuracy: 0.6479

Epoch 00072: val_loss did not improve from 0.61112
Epoch 73/1000
36/36 [==============================] - 330s 9s/step - loss: 0.6143 - accuracy: 0.4745 - val_loss: 0.6272 - val_accuracy: 0.5298

Epoch 00073: val_loss did not improve from 0.61112
Epoch 74/1000
36/36 [==============================] - 341s 10s/step - loss: 0.6089 - accuracy: 0.4345 - val_loss: 0.6069 - val_accuracy: 0.2745

Epoch 00074: val_loss improved from 0.61112 to 0.60687, saving model to model-proba4-4.h5
Epoch 75/1000
36/36 [==============================] - 328s 9s/step - loss: 0.6176 - accuracy: 0.3769 - val_loss: 0.6093 - val_accuracy: 0.4225

Epoch 00075: val_loss did not improve from 0.60687
Epoch 76/1000
36/36 [==============================] - 337s 9s/step - loss: 0.6082 - accuracy: 0.4272 - val_loss: 0.6058 - val_accuracy: 0.3325

Epoch 00076: val_loss improved from 0.60687 to 0.60584, saving model to model-proba4-4.h5
Epoch 77/1000
36/36 [==============================] - 323s 9s/step - loss: 0.6153 - accuracy: 0.4393 - val_loss: 0.6213 - val_accuracy: 0.5003

Epoch 00077: val_loss did not improve from 0.60584
Epoch 78/1000
36/36 [==============================] - 337s 9s/step - loss: 0.6114 - accuracy: 0.3917 - val_loss: 0.6091 - val_accuracy: 0.3916

Epoch 00078: val_loss did not improve from 0.60584
Epoch 79/1000
36/36 [==============================] - 335s 9s/step - loss: 0.6077 - accuracy: 0.3869 - val_loss: 0.6205 - val_accuracy: 0.3292

Epoch 00079: val_loss did not improve from 0.60584
Epoch 80/1000
36/36 [==============================] - 338s 9s/step - loss: 0.6056 - accuracy: 0.3534 - val_loss: 0.6041 - val_accuracy: 0.3481

Epoch 00080: val_loss improved from 0.60584 to 0.60407, saving model to model-proba4-4.h5
Epoch 81/1000
36/36 [==============================] - 336s 9s/step - loss: 0.6052 - accuracy: 0.3833 - val_loss: 0.6112 - val_accuracy: 0.2915

Epoch 00081: val_loss did not improve from 0.60407
Epoch 82/1000
36/36 [==============================] - 341s 9s/step - loss: 0.6059 - accuracy: 0.3743 - val_loss: 0.6280 - val_accuracy: 0.4190

Epoch 00082: val_loss did not improve from 0.60407
Epoch 83/1000
36/36 [==============================] - 323s 9s/step - loss: 0.6127 - accuracy: 0.3995 - val_loss: 0.6148 - val_accuracy: 0.4517

Epoch 00083: val_loss did not improve from 0.60407
Epoch 84/1000
36/36 [==============================] - 324s 9s/step - loss: 0.6126 - accuracy: 0.4003 - val_loss: 0.6149 - val_accuracy: 0.4876

Epoch 00084: val_loss did not improve from 0.60407
Epoch 85/1000
36/36 [==============================] - 329s 9s/step - loss: 0.6090 - accuracy: 0.4069 - val_loss: 0.6119 - val_accuracy: 0.3967

Epoch 00085: val_loss did not improve from 0.60407
Epoch 86/1000
36/36 [==============================] - 335s 9s/step - loss: 0.6082 - accuracy: 0.3741 - val_loss: 0.6147 - val_accuracy: 0.5020

Epoch 00086: val_loss did not improve from 0.60407
Epoch 87/1000
36/36 [==============================] - 322s 9s/step - loss: 0.6134 - accuracy: 0.4569 - val_loss: 0.6093 - val_accuracy: 0.4155

Epoch 00087: val_loss did not improve from 0.60407
Epoch 88/1000
36/36 [==============================] - 348s 10s/step - loss: 0.6079 - accuracy: 0.4092 - val_loss: 0.6284 - val_accuracy: 0.5310

Epoch 00088: val_loss did not improve from 0.60407
Epoch 89/1000
36/36 [==============================] - 316s 9s/step - loss: 0.6118 - accuracy: 0.3971 - val_loss: 0.6112 - val_accuracy: 0.3350

Epoch 00089: val_loss did not improve from 0.60407
Epoch 90/1000
36/36 [==============================] - 307s 9s/step - loss: 0.6118 - accuracy: 0.3732 - val_loss: 0.6156 - val_accuracy: 0.4184

Epoch 00090: val_loss did not improve from 0.60407
Epoch 91/1000
36/36 [==============================] - 332s 9s/step - loss: 0.6034 - accuracy: 0.3946 - val_loss: 0.6090 - val_accuracy: 0.4154

Epoch 00091: val_loss did not improve from 0.60407
Epoch 92/1000
36/36 [==============================] - 334s 9s/step - loss: 0.6086 - accuracy: 0.4015 - val_loss: 0.6108 - val_accuracy: 0.3473

Epoch 00092: val_loss did not improve from 0.60407
Epoch 93/1000
36/36 [==============================] - 324s 9s/step - loss: 0.6055 - accuracy: 0.3863 - val_loss: 0.6038 - val_accuracy: 0.2593

Epoch 00093: val_loss improved from 0.60407 to 0.60380, saving model to model-proba4-4.h5
Epoch 94/1000
36/36 [==============================] - 322s 9s/step - loss: 0.6037 - accuracy: 0.3530 - val_loss: 0.6081 - val_accuracy: 0.3481

Epoch 00094: val_loss did not improve from 0.60380
Epoch 95/1000
36/36 [==============================] - 658s 18s/step - loss: 0.6083 - accuracy: 0.3938 - val_loss: 0.6053 - val_accuracy: 0.4448

Epoch 00095: val_loss did not improve from 0.60380
Epoch 96/1000
36/36 [==============================] - 704s 19s/step - loss: 0.6048 - accuracy: 0.4088 - val_loss: 0.6200 - val_accuracy: 0.3185

Epoch 00096: val_loss did not improve from 0.60380
Epoch 97/1000
36/36 [==============================] - 632s 17s/step - loss: 0.6017 - accuracy: 0.3251 - val_loss: 0.6111 - val_accuracy: 0.3091

Epoch 00097: val_loss did not improve from 0.60380
Epoch 98/1000
36/36 [==============================] - 618s 17s/step - loss: 0.6076 - accuracy: 0.3130 - val_loss: 0.6219 - val_accuracy: 0.4242

Epoch 00098: val_loss did not improve from 0.60380
Epoch 99/1000
36/36 [==============================] - 621s 17s/step - loss: 0.6034 - accuracy: 0.3510 - val_loss: 0.6299 - val_accuracy: 0.4470

Epoch 00099: val_loss did not improve from 0.60380
Epoch 100/1000
36/36 [==============================] - 606s 17s/step - loss: 0.6097 - accuracy: 0.4094 - val_loss: 0.6071 - val_accuracy: 0.4259

Epoch 00100: val_loss did not improve from 0.60380
Epoch 101/1000
36/36 [==============================] - 571s 16s/step - loss: 0.6073 - accuracy: 0.3942 - val_loss: 0.6119 - val_accuracy: 0.3781

Epoch 00101: val_loss did not improve from 0.60380
Epoch 102/1000
36/36 [==============================] - 311s 9s/step - loss: 0.6077 - accuracy: 0.3803 - val_loss: 0.6019 - val_accuracy: 0.2551

Epoch 00102: val_loss improved from 0.60380 to 0.60187, saving model to model-proba4-4.h5
Epoch 103/1000
36/36 [==============================] - 319s 9s/step - loss: 0.6019 - accuracy: 0.3684 - val_loss: 0.6011 - val_accuracy: 0.4004

Epoch 00103: val_loss improved from 0.60187 to 0.60111, saving model to model-proba4-4.h5
Epoch 104/1000
36/36 [==============================] - 327s 9s/step - loss: 0.6072 - accuracy: 0.3535 - val_loss: 0.6208 - val_accuracy: 0.2703

Epoch 00104: val_loss did not improve from 0.60111
Epoch 105/1000
36/36 [==============================] - 312s 9s/step - loss: 0.6114 - accuracy: 0.3951 - val_loss: 0.6193 - val_accuracy: 0.4719

Epoch 00105: val_loss did not improve from 0.60111
Epoch 106/1000
36/36 [==============================] - 316s 9s/step - loss: 0.6066 - accuracy: 0.4194 - val_loss: 0.6003 - val_accuracy: 0.2623

Epoch 00106: val_loss improved from 0.60111 to 0.60032, saving model to model-proba4-4.h5
Epoch 107/1000
36/36 [==============================] - 310s 8s/step - loss: 0.6073 - accuracy: 0.3614 - val_loss: 0.6075 - val_accuracy: 0.3497

Epoch 00107: val_loss did not improve from 0.60032
Epoch 108/1000
36/36 [==============================] - 314s 9s/step - loss: 0.6055 - accuracy: 0.3571 - val_loss: 0.6055 - val_accuracy: 0.1863

Epoch 00108: val_loss did not improve from 0.60032
Epoch 109/1000
36/36 [==============================] - 332s 9s/step - loss: 0.6105 - accuracy: 0.3615 - val_loss: 0.6154 - val_accuracy: 0.4386

Epoch 00109: val_loss did not improve from 0.60032
Epoch 110/1000
36/36 [==============================] - 317s 9s/step - loss: 0.6050 - accuracy: 0.3807 - val_loss: 0.6153 - val_accuracy: 0.4181

Epoch 00110: val_loss did not improve from 0.60032
Epoch 111/1000
36/36 [==============================] - 324s 9s/step - loss: 0.6098 - accuracy: 0.3835 - val_loss: 0.6130 - val_accuracy: 0.3678

Epoch 00111: val_loss did not improve from 0.60032
Epoch 112/1000
36/36 [==============================] - 310s 9s/step - loss: 0.6048 - accuracy: 0.3582 - val_loss: 0.6288 - val_accuracy: 0.4813

Epoch 00112: val_loss did not improve from 0.60032
Epoch 113/1000
36/36 [==============================] - 307s 8s/step - loss: 0.6017 - accuracy: 0.3669 - val_loss: 0.6099 - val_accuracy: 0.4055

Epoch 00113: val_loss did not improve from 0.60032
Epoch 114/1000
36/36 [==============================] - 309s 9s/step - loss: 0.5982 - accuracy: 0.3396 - val_loss: 0.6058 - val_accuracy: 0.3031

Epoch 00114: val_loss did not improve from 0.60032
Epoch 115/1000
36/36 [==============================] - 316s 9s/step - loss: 0.6010 - accuracy: 0.3720 - val_loss: 0.6426 - val_accuracy: 0.2831

Epoch 00115: val_loss did not improve from 0.60032
Epoch 116/1000
^C
exit
eTraceback (most recent call last):
  File "proba4.py", line 206, in <module>
    results = model.fit(train_gen, batch_size=batch_size, epochs=1000, callbacks=callbacks, validation_data=(val_gen))
  File "/home/veronika/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 1100, in fit
    tmp_logs = self.train_function(iterator)
  File "/home/veronika/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 828, in __call__
    result = self._call(*args, **kwds)
  File "/home/veronika/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 855, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File "/home/veronika/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 2942, in __call__
    return graph_function._call_flat(
  File "/home/veronika/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 1918, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File "/home/veronika/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 555, in call
    outputs = execute.execute(
  File "/home/veronika/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
KeyboardInterrupt
2021-04-09 18:09:23.208049: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]

veronika@kamis-nn01:~/radiIzvanka_QAApproved/probe$ 
veronika@kamis-nn01:~/radiIzvanka_QAApproved/probe$ exit
logout
Connection to kamis-nn01.fesb.hr closed.
(base) Veronikas-MacBook-Pro:~ veronika$ ssh veronika@kamis-nn01.fesb.hr -p 10003
veronika@kamis-nn01.fesb.hr's password: 
Welcome to Ubuntu 20.04.1 LTS (GNU/Linux 5.4.0-65-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage
Last login: Fri Apr  9 15:38:57 2021 from 127.0.0.1
veronika@kamis-nn01:~$ cd radiIzvanka_QAApproved/
veronika@kamis-nn01:~/radiIzvanka_QAApproved$ cd probe/
veronika@kamis-nn01:~/radiIzvanka_QAApproved/probe$ ls
model-proba2.h5  model-proba4-2.h5  model-proba4-3.h5  model-proba4-4.h5  model-proba4.h5  model-proba5-1.h5  proba3.py  proba4.py  proba5.py
veronika@kamis-nn01:~/radiIzvanka_QAApproved/probe$ vim proba5.py 
veronika@kamis-nn01:~/radiIzvanka_QAApproved/probe$ python3 proba5.py 
2021-04-09 18:10:45.173655: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-04-09 18:10:45.173698: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-04-09 18:10:48.141010: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-09 18:10:48.142256: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-04-09 18:10:48.182073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-04-09 18:10:48.182873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: 
pciBusID: 0000:81:00.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-04-09 18:10:48.182933: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-04-09 18:10:48.183116: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory
2021-04-09 18:10:48.183170: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory
2021-04-09 18:10:48.183212: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2021-04-09 18:10:48.183269: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2021-04-09 18:10:48.183302: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory
2021-04-09 18:10:48.183335: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory
2021-04-09 18:10:48.183373: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory
2021-04-09 18:10:48.183382: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2021-04-09 18:10:48.183642: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-04-09 18:10:48.188659: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-09 18:10:48.188682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-04-09 18:10:48.188690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
img (InputLayer)                [(None, 512, 512, 3) 0                                            
__________________________________________________________________________________________________
separable_conv2d (SeparableConv (None, 512, 512, 16) 91          img[0][0]                        
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 512, 512, 16) 64          separable_conv2d[0][0]           
__________________________________________________________________________________________________
activation (Activation)         (None, 512, 512, 16) 0           batch_normalization[0][0]        
__________________________________________________________________________________________________
separable_conv2d_1 (SeparableCo (None, 512, 512, 16) 416         activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 512, 512, 16) 64          separable_conv2d_1[0][0]         
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 512, 512, 16) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
separable_conv2d_2 (SeparableCo (None, 512, 512, 16) 416         activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 512, 512, 16) 64          separable_conv2d_2[0][0]         
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 512, 512, 16) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 256, 256, 16) 0           activation_2[0][0]               
__________________________________________________________________________________________________
separable_conv2d_3 (SeparableCo (None, 256, 256, 32) 688         max_pooling2d[0][0]              
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 256, 256, 32) 128         separable_conv2d_3[0][0]         
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 256, 256, 32) 0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
separable_conv2d_4 (SeparableCo (None, 256, 256, 32) 1344        activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 256, 256, 32) 128         separable_conv2d_4[0][0]         
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 256, 256, 32) 0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
separable_conv2d_5 (SeparableCo (None, 256, 256, 32) 1344        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 256, 256, 32) 128         separable_conv2d_5[0][0]         
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 256, 256, 32) 0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 32) 0           activation_5[0][0]               
__________________________________________________________________________________________________
separable_conv2d_6 (SeparableCo (None, 128, 128, 64) 2400        max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 128, 128, 64) 256         separable_conv2d_6[0][0]         
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 128, 128, 64) 0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
separable_conv2d_7 (SeparableCo (None, 128, 128, 64) 4736        activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 128, 128, 64) 256         separable_conv2d_7[0][0]         
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 128, 128, 64) 0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
separable_conv2d_8 (SeparableCo (None, 128, 128, 64) 4736        activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 128, 128, 64) 256         separable_conv2d_8[0][0]         
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 128, 128, 64) 0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 64)   0           activation_8[0][0]               
__________________________________________________________________________________________________
separable_conv2d_9 (SeparableCo (None, 64, 64, 128)  8896        max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 64, 64, 128)  512         separable_conv2d_9[0][0]         
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 64, 64, 128)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
separable_conv2d_10 (SeparableC (None, 64, 64, 128)  17664       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 64, 64, 128)  512         separable_conv2d_10[0][0]        
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 64, 64, 128)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
separable_conv2d_11 (SeparableC (None, 64, 64, 128)  17664       activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 64, 64, 128)  512         separable_conv2d_11[0][0]        
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 64, 64, 128)  0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 128)  0           activation_11[0][0]              
__________________________________________________________________________________________________
separable_conv2d_12 (SeparableC (None, 32, 32, 256)  34176       max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 32, 32, 256)  1024        separable_conv2d_12[0][0]        
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 32, 32, 256)  0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
separable_conv2d_13 (SeparableC (None, 32, 32, 256)  68096       activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 32, 32, 256)  1024        separable_conv2d_13[0][0]        
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 32, 32, 256)  0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
separable_conv2d_14 (SeparableC (None, 32, 32, 256)  68096       activation_13[0][0]              
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 32, 32, 256)  1024        separable_conv2d_14[0][0]        
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 32, 32, 256)  0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 16, 16, 256)  0           activation_14[0][0]              
__________________________________________________________________________________________________
separable_conv2d_15 (SeparableC (None, 16, 16, 512)  133888      max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 16, 16, 512)  2048        separable_conv2d_15[0][0]        
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 16, 16, 512)  0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
separable_conv2d_16 (SeparableC (None, 16, 16, 512)  267264      activation_15[0][0]              
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 16, 16, 512)  2048        separable_conv2d_16[0][0]        
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 16, 16, 512)  0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
separable_conv2d_17 (SeparableC (None, 16, 16, 512)  267264      activation_16[0][0]              
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 16, 16, 512)  2048        separable_conv2d_17[0][0]        
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 16, 16, 512)  0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
conv2d_transpose (Conv2DTranspo (None, 32, 32, 256)  1179904     activation_17[0][0]              
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 32, 32, 512)  0           conv2d_transpose[0][0]           
                                                                 activation_14[0][0]              
__________________________________________________________________________________________________
separable_conv2d_18 (SeparableC (None, 32, 32, 128)  70272       concatenate[0][0]                
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 32, 32, 128)  512         separable_conv2d_18[0][0]        
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 32, 32, 128)  0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
separable_conv2d_19 (SeparableC (None, 32, 32, 128)  17664       activation_18[0][0]              
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 32, 32, 128)  512         separable_conv2d_19[0][0]        
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 32, 32, 128)  0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
separable_conv2d_20 (SeparableC (None, 32, 32, 128)  17664       activation_19[0][0]              
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 32, 32, 128)  512         separable_conv2d_20[0][0]        
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 32, 32, 128)  0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 64, 64, 128)  147584      activation_20[0][0]              
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 64, 64, 256)  0           conv2d_transpose_1[0][0]         
                                                                 activation_11[0][0]              
__________________________________________________________________________________________________
separable_conv2d_21 (SeparableC (None, 64, 64, 128)  35200       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 64, 64, 128)  512         separable_conv2d_21[0][0]        
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 64, 64, 128)  0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
separable_conv2d_22 (SeparableC (None, 64, 64, 128)  17664       activation_21[0][0]              
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 64, 64, 128)  512         separable_conv2d_22[0][0]        
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 64, 64, 128)  0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
separable_conv2d_23 (SeparableC (None, 64, 64, 128)  17664       activation_22[0][0]              
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 64, 64, 128)  512         separable_conv2d_23[0][0]        
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 64, 64, 128)  0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 128, 128, 64) 73792       activation_23[0][0]              
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 128, 128, 128 0           conv2d_transpose_2[0][0]         
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
separable_conv2d_24 (SeparableC (None, 128, 128, 64) 9408        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 128, 128, 64) 256         separable_conv2d_24[0][0]        
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 128, 128, 64) 0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
separable_conv2d_25 (SeparableC (None, 128, 128, 64) 4736        activation_24[0][0]              
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 128, 128, 64) 256         separable_conv2d_25[0][0]        
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 128, 128, 64) 0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
separable_conv2d_26 (SeparableC (None, 128, 128, 64) 4736        activation_25[0][0]              
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 128, 128, 64) 256         separable_conv2d_26[0][0]        
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 128, 128, 64) 0           batch_normalization_26[0][0]     
__________________________________________________________________________________________________
conv2d_transpose_3 (Conv2DTrans (None, 256, 256, 32) 18464       activation_26[0][0]              
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 256, 256, 64) 0           conv2d_transpose_3[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
separable_conv2d_27 (SeparableC (None, 256, 256, 32) 2656        concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 256, 256, 32) 128         separable_conv2d_27[0][0]        
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 256, 256, 32) 0           batch_normalization_27[0][0]     
__________________________________________________________________________________________________
separable_conv2d_28 (SeparableC (None, 256, 256, 32) 1344        activation_27[0][0]              
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 256, 256, 32) 128         separable_conv2d_28[0][0]        
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 256, 256, 32) 0           batch_normalization_28[0][0]     
__________________________________________________________________________________________________
separable_conv2d_29 (SeparableC (None, 256, 256, 32) 1344        activation_28[0][0]              
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 256, 256, 32) 128         separable_conv2d_29[0][0]        
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 256, 256, 32) 0           batch_normalization_29[0][0]     
__________________________________________________________________________________________________
conv2d_transpose_4 (Conv2DTrans (None, 512, 512, 16) 4624        activation_29[0][0]              
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 512, 512, 32) 0           conv2d_transpose_4[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
separable_conv2d_30 (SeparableC (None, 512, 512, 16) 816         concatenate_4[0][0]              
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 512, 512, 16) 64          separable_conv2d_30[0][0]        
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 512, 512, 16) 0           batch_normalization_30[0][0]     
__________________________________________________________________________________________________
separable_conv2d_31 (SeparableC (None, 512, 512, 16) 416         activation_30[0][0]              
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 512, 512, 16) 64          separable_conv2d_31[0][0]        
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 512, 512, 16) 0           batch_normalization_31[0][0]     
__________________________________________________________________________________________________
separable_conv2d_32 (SeparableC (None, 512, 512, 16) 416         activation_31[0][0]              
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 512, 512, 16) 64          separable_conv2d_32[0][0]        
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 512, 512, 16) 0           batch_normalization_32[0][0]     
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 512, 512, 3)  51          activation_32[0][0]              
==================================================================================================
Total params: 2,542,110
Trainable params: 2,533,854
Non-trainable params: 8,256
__________________________________________________________________________________________________
None
2021-04-09 18:10:49.719731: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-04-09 18:10:49.741033: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2999955000 Hz
Epoch 1/500
36/36 [==============================] - 525s 14s/step - loss: 0.6787 - accuracy: 0.1097 - val_loss: 0.6858 - val_accuracy: 0.9999

Epoch 00001: val_loss improved from inf to 0.68581, saving model to model-proba5-2.h5
Epoch 2/500
36/36 [==============================] - 522s 15s/step - loss: 0.6313 - accuracy: 0.2694 - val_loss: 0.6818 - val_accuracy: 0.0000e+00

Epoch 00002: val_loss improved from 0.68581 to 0.68183, saving model to model-proba5-2.h5
Epoch 3/500
36/36 [==============================] - 508s 14s/step - loss: 0.6251 - accuracy: 0.2732 - val_loss: 0.6812 - val_accuracy: 0.0000e+00

Epoch 00003: val_loss improved from 0.68183 to 0.68120, saving model to model-proba5-2.h5
Epoch 4/500
36/36 [==============================] - 516s 14s/step - loss: 0.6154 - accuracy: 0.2934 - val_loss: 0.6788 - val_accuracy: 0.0000e+00

Epoch 00004: val_loss improved from 0.68120 to 0.67884, saving model to model-proba5-2.h5
Epoch 5/500
36/36 [==============================] - 496s 14s/step - loss: 0.6104 - accuracy: 0.2960 - val_loss: 0.6761 - val_accuracy: 0.0000e+00

Epoch 00005: val_loss improved from 0.67884 to 0.67611, saving model to model-proba5-2.h5
Epoch 6/500
36/36 [==============================] - 503s 14s/step - loss: 0.6120 - accuracy: 0.3173 - val_loss: 0.6742 - val_accuracy: 0.0019

Epoch 00006: val_loss improved from 0.67611 to 0.67423, saving model to model-proba5-2.h5
Epoch 7/500
36/36 [==============================] - 525s 14s/step - loss: 0.6112 - accuracy: 0.3340 - val_loss: 0.6778 - val_accuracy: 0.0038

Epoch 00007: val_loss did not improve from 0.67423
Epoch 8/500
36/36 [==============================] - 500s 14s/step - loss: 0.6028 - accuracy: 0.3092 - val_loss: 0.6793 - val_accuracy: 0.1804

Epoch 00008: val_loss did not improve from 0.67423
Epoch 9/500
36/36 [==============================] - 526s 15s/step - loss: 0.5992 - accuracy: 0.2753 - val_loss: 0.6717 - val_accuracy: 0.9887

Epoch 00009: val_loss improved from 0.67423 to 0.67166, saving model to model-proba5-2.h5
Epoch 10/500
36/36 [==============================] - 520s 14s/step - loss: 0.5949 - accuracy: 0.2916 - val_loss: 0.6629 - val_accuracy: 0.9905

Epoch 00010: val_loss improved from 0.67166 to 0.66290, saving model to model-proba5-2.h5
Epoch 11/500
36/36 [==============================] - 502s 14s/step - loss: 0.5908 - accuracy: 0.3283 - val_loss: 0.6612 - val_accuracy: 0.9661

Epoch 00011: val_loss improved from 0.66290 to 0.66121, saving model to model-proba5-2.h5
Epoch 12/500
36/36 [==============================] - 503s 14s/step - loss: 0.5898 - accuracy: 0.3125 - val_loss: 0.6486 - val_accuracy: 0.9325

Epoch 00012: val_loss improved from 0.66121 to 0.64856, saving model to model-proba5-2.h5
Epoch 13/500
36/36 [==============================] - 502s 14s/step - loss: 0.5874 - accuracy: 0.2967 - val_loss: 0.6447 - val_accuracy: 0.8593

Epoch 00013: val_loss improved from 0.64856 to 0.64474, saving model to model-proba5-2.h5
Epoch 14/500
36/36 [==============================] - 503s 14s/step - loss: 0.5812 - accuracy: 0.2558 - val_loss: 0.6403 - val_accuracy: 0.8292

Epoch 00014: val_loss improved from 0.64474 to 0.64027, saving model to model-proba5-2.h5
Epoch 15/500
36/36 [==============================] - 491s 13s/step - loss: 0.5734 - accuracy: 0.2926 - val_loss: 0.6596 - val_accuracy: 0.6853

Epoch 00015: val_loss did not improve from 0.64027
Epoch 16/500
36/36 [==============================] - 496s 14s/step - loss: 0.5677 - accuracy: 0.2575 - val_loss: 0.6532 - val_accuracy: 0.6894

Epoch 00016: val_loss did not improve from 0.64027
Epoch 17/500
36/36 [==============================] - 523s 14s/step - loss: 0.5653 - accuracy: 0.2931 - val_loss: 0.6898 - val_accuracy: 0.5664

Epoch 00017: val_loss did not improve from 0.64027
Epoch 18/500
36/36 [==============================] - 495s 14s/step - loss: 0.5584 - accuracy: 0.2819 - val_loss: 0.6348 - val_accuracy: 0.4796

Epoch 00018: val_loss improved from 0.64027 to 0.63483, saving model to model-proba5-2.h5
Epoch 19/500
36/36 [==============================] - 531s 15s/step - loss: 0.5477 - accuracy: 0.2740 - val_loss: 0.6072 - val_accuracy: 0.3796

Epoch 00019: val_loss improved from 0.63483 to 0.60720, saving model to model-proba5-2.h5
Epoch 20/500
36/36 [==============================] - 506s 14s/step - loss: 0.5494 - accuracy: 0.2776 - val_loss: 0.5936 - val_accuracy: 0.4426

Epoch 00020: val_loss improved from 0.60720 to 0.59355, saving model to model-proba5-2.h5
Epoch 21/500
36/36 [==============================] - 515s 14s/step - loss: 0.5435 - accuracy: 0.3682 - val_loss: 0.6145 - val_accuracy: 0.3231

Epoch 00021: val_loss did not improve from 0.59355
Epoch 22/500
36/36 [==============================] - 534s 15s/step - loss: 0.5445 - accuracy: 0.2964 - val_loss: 0.7697 - val_accuracy: 0.3129

Epoch 00022: val_loss did not improve from 0.59355
Epoch 23/500
36/36 [==============================] - 538s 15s/step - loss: 0.5405 - accuracy: 0.2659 - val_loss: 0.6027 - val_accuracy: 0.4278

Epoch 00023: val_loss did not improve from 0.59355
Epoch 24/500
36/36 [==============================] - 536s 15s/step - loss: 0.5311 - accuracy: 0.3510 - val_loss: 0.7694 - val_accuracy: 0.4429

Epoch 00024: val_loss did not improve from 0.59355
Epoch 25/500
36/36 [==============================] - 553s 15s/step - loss: 0.5260 - accuracy: 0.3528 - val_loss: 0.7723 - val_accuracy: 0.2846

Epoch 00025: val_loss did not improve from 0.59355
Epoch 26/500
36/36 [==============================] - 521s 14s/step - loss: 0.5184 - accuracy: 0.2632 - val_loss: 0.6308 - val_accuracy: 0.1564

Epoch 00026: val_loss did not improve from 0.59355
Epoch 27/500
36/36 [==============================] - 539s 15s/step - loss: 0.5219 - accuracy: 0.3125 - val_loss: 0.7069 - val_accuracy: 0.1461

Epoch 00027: val_loss did not improve from 0.59355
Epoch 28/500
36/36 [==============================] - 504s 14s/step - loss: 0.5183 - accuracy: 0.3169 - val_loss: 0.6654 - val_accuracy: 0.1836

Epoch 00028: val_loss did not improve from 0.59355
Epoch 29/500
36/36 [==============================] - 532s 15s/step - loss: 0.5142 - accuracy: 0.2701 - val_loss: 0.7696 - val_accuracy: 0.2716

Epoch 00029: val_loss did not improve from 0.59355
Epoch 30/500
36/36 [==============================] - 521s 15s/step - loss: 0.5049 - accuracy: 0.2718 - val_loss: 0.7248 - val_accuracy: 0.5006

Epoch 00030: val_loss did not improve from 0.59355
Epoch 31/500
36/36 [==============================] - 521s 14s/step - loss: 0.5031 - accuracy: 0.2607 - val_loss: 0.6465 - val_accuracy: 0.3383

Epoch 00031: val_loss did not improve from 0.59355
Epoch 32/500
36/36 [==============================] - 531s 15s/step - loss: 0.5077 - accuracy: 0.2395 - val_loss: 0.6129 - val_accuracy: 0.3155

Epoch 00032: val_loss did not improve from 0.59355
Epoch 33/500
36/36 [==============================] - 509s 14s/step - loss: 0.5035 - accuracy: 0.3137 - val_loss: 0.6513 - val_accuracy: 0.2787

Epoch 00033: val_loss did not improve from 0.59355
Epoch 34/500
36/36 [==============================] - 511s 14s/step - loss: 0.5015 - accuracy: 0.3200 - val_loss: 0.6031 - val_accuracy: 0.3127

Epoch 00034: val_loss did not improve from 0.59355
Epoch 35/500
36/36 [==============================] - 508s 14s/step - loss: 0.5002 - accuracy: 0.3154 - val_loss: 0.5998 - val_accuracy: 0.3825

Epoch 00035: val_loss did not improve from 0.59355
Epoch 36/500
36/36 [==============================] - 507s 14s/step - loss: 0.4983 - accuracy: 0.3634 - val_loss: 0.6219 - val_accuracy: 0.1982

Epoch 00036: val_loss did not improve from 0.59355
Epoch 37/500
36/36 [==============================] - 508s 14s/step - loss: 0.4936 - accuracy: 0.2714 - val_loss: 0.6107 - val_accuracy: 0.3608

Epoch 00037: val_loss did not improve from 0.59355
Epoch 38/500
36/36 [==============================] - 518s 14s/step - loss: 0.4948 - accuracy: 0.3639 - val_loss: 0.6209 - val_accuracy: 0.1945

Epoch 00038: val_loss did not improve from 0.59355
Epoch 39/500
26/36 [====================>.........] - ETA: 2:25 - loss: 0.4829 - accuracy: 0.2728packet_write_wait: Connection to 161.53.171.105 port 10003: Broken pipe
(base) Veronikas-MacBook-Pro:~ veronika$ ssh veronika@kamis-nn01.fesb.hr -p 10003
veronika@kamis-nn01.fesb.hr's password: 
Welcome to Ubuntu 20.04.1 LTS (GNU/Linux 5.4.0-65-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage
Last login: Fri Apr  9 18:09:55 2021 from 127.0.0.1
veronika@kamis-nn01:~$ cd radiIzvanka_QAApproved/
veronika@kamis-nn01:~/radiIzvanka_QAApproved$ cd probe
veronika@kamis-nn01:~/radiIzvanka_QAApproved/probe$ vim proba5.py 
veronika@kamis-nn01:~/radiIzvanka_QAApproved/probe$ python3 proba5.py 
2021-04-10 07:37:03.860615: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-04-10 07:37:03.860662: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-04-10 07:37:06.668058: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-10 07:37:06.669469: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-04-10 07:37:06.714097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-04-10 07:37:06.714941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: 
pciBusID: 0000:81:00.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-04-10 07:37:06.715041: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-04-10 07:37:06.715100: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory
2021-04-10 07:37:06.715137: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory
2021-04-10 07:37:06.715173: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2021-04-10 07:37:06.715211: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2021-04-10 07:37:06.715244: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory
2021-04-10 07:37:06.715275: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory
2021-04-10 07:37:06.715311: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory
2021-04-10 07:37:06.715319: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2021-04-10 07:37:06.715619: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-04-10 07:37:06.716129: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-10 07:37:06.716153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-04-10 07:37:06.716163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
img (InputLayer)                [(None, 512, 512, 3) 0                                            
__________________________________________________________________________________________________
separable_conv2d (SeparableConv (None, 512, 512, 16) 91          img[0][0]                        
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 512, 512, 16) 64          separable_conv2d[0][0]           
__________________________________________________________________________________________________
activation (Activation)         (None, 512, 512, 16) 0           batch_normalization[0][0]        
__________________________________________________________________________________________________
separable_conv2d_1 (SeparableCo (None, 512, 512, 16) 416         activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 512, 512, 16) 64          separable_conv2d_1[0][0]         
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 512, 512, 16) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
separable_conv2d_2 (SeparableCo (None, 512, 512, 16) 416         activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 512, 512, 16) 64          separable_conv2d_2[0][0]         
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 512, 512, 16) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 256, 256, 16) 0           activation_2[0][0]               
__________________________________________________________________________________________________
separable_conv2d_3 (SeparableCo (None, 256, 256, 32) 688         max_pooling2d[0][0]              
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 256, 256, 32) 128         separable_conv2d_3[0][0]         
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 256, 256, 32) 0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
separable_conv2d_4 (SeparableCo (None, 256, 256, 32) 1344        activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 256, 256, 32) 128         separable_conv2d_4[0][0]         
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 256, 256, 32) 0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
separable_conv2d_5 (SeparableCo (None, 256, 256, 32) 1344        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 256, 256, 32) 128         separable_conv2d_5[0][0]         
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 256, 256, 32) 0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 32) 0           activation_5[0][0]               
__________________________________________________________________________________________________
separable_conv2d_6 (SeparableCo (None, 128, 128, 64) 2400        max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 128, 128, 64) 256         separable_conv2d_6[0][0]         
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 128, 128, 64) 0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
separable_conv2d_7 (SeparableCo (None, 128, 128, 64) 4736        activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 128, 128, 64) 256         separable_conv2d_7[0][0]         
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 128, 128, 64) 0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
separable_conv2d_8 (SeparableCo (None, 128, 128, 64) 4736        activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 128, 128, 64) 256         separable_conv2d_8[0][0]         
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 128, 128, 64) 0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 64)   0           activation_8[0][0]               
__________________________________________________________________________________________________
separable_conv2d_9 (SeparableCo (None, 64, 64, 128)  8896        max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 64, 64, 128)  512         separable_conv2d_9[0][0]         
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 64, 64, 128)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
separable_conv2d_10 (SeparableC (None, 64, 64, 128)  17664       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 64, 64, 128)  512         separable_conv2d_10[0][0]        
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 64, 64, 128)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
separable_conv2d_11 (SeparableC (None, 64, 64, 128)  17664       activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 64, 64, 128)  512         separable_conv2d_11[0][0]        
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 64, 64, 128)  0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 128)  0           activation_11[0][0]              
__________________________________________________________________________________________________
separable_conv2d_12 (SeparableC (None, 32, 32, 256)  34176       max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 32, 32, 256)  1024        separable_conv2d_12[0][0]        
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 32, 32, 256)  0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
separable_conv2d_13 (SeparableC (None, 32, 32, 256)  68096       activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 32, 32, 256)  1024        separable_conv2d_13[0][0]        
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 32, 32, 256)  0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
separable_conv2d_14 (SeparableC (None, 32, 32, 256)  68096       activation_13[0][0]              
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 32, 32, 256)  1024        separable_conv2d_14[0][0]        
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 32, 32, 256)  0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 16, 16, 256)  0           activation_14[0][0]              
__________________________________________________________________________________________________
separable_conv2d_15 (SeparableC (None, 16, 16, 512)  133888      max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 16, 16, 512)  2048        separable_conv2d_15[0][0]        
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 16, 16, 512)  0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
separable_conv2d_16 (SeparableC (None, 16, 16, 512)  267264      activation_15[0][0]              
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 16, 16, 512)  2048        separable_conv2d_16[0][0]        
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 16, 16, 512)  0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
separable_conv2d_17 (SeparableC (None, 16, 16, 512)  267264      activation_16[0][0]              
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 16, 16, 512)  2048        separable_conv2d_17[0][0]        
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 16, 16, 512)  0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
conv2d_transpose (Conv2DTranspo (None, 32, 32, 256)  1179904     activation_17[0][0]              
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 32, 32, 512)  0           conv2d_transpose[0][0]           
                                                                 activation_14[0][0]              
__________________________________________________________________________________________________
separable_conv2d_18 (SeparableC (None, 32, 32, 128)  70272       concatenate[0][0]                
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 32, 32, 128)  512         separable_conv2d_18[0][0]        
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 32, 32, 128)  0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
separable_conv2d_19 (SeparableC (None, 32, 32, 128)  17664       activation_18[0][0]              
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 32, 32, 128)  512         separable_conv2d_19[0][0]        
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 32, 32, 128)  0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
separable_conv2d_20 (SeparableC (None, 32, 32, 128)  17664       activation_19[0][0]              
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 32, 32, 128)  512         separable_conv2d_20[0][0]        
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 32, 32, 128)  0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 64, 64, 128)  147584      activation_20[0][0]              
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 64, 64, 256)  0           conv2d_transpose_1[0][0]         
                                                                 activation_11[0][0]              
__________________________________________________________________________________________________
separable_conv2d_21 (SeparableC (None, 64, 64, 128)  35200       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 64, 64, 128)  512         separable_conv2d_21[0][0]        
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 64, 64, 128)  0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
separable_conv2d_22 (SeparableC (None, 64, 64, 128)  17664       activation_21[0][0]              
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 64, 64, 128)  512         separable_conv2d_22[0][0]        
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 64, 64, 128)  0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
separable_conv2d_23 (SeparableC (None, 64, 64, 128)  17664       activation_22[0][0]              
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 64, 64, 128)  512         separable_conv2d_23[0][0]        
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 64, 64, 128)  0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 128, 128, 64) 73792       activation_23[0][0]              
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 128, 128, 128 0           conv2d_transpose_2[0][0]         
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
separable_conv2d_24 (SeparableC (None, 128, 128, 64) 9408        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 128, 128, 64) 256         separable_conv2d_24[0][0]        
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 128, 128, 64) 0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
separable_conv2d_25 (SeparableC (None, 128, 128, 64) 4736        activation_24[0][0]              
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 128, 128, 64) 256         separable_conv2d_25[0][0]        
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 128, 128, 64) 0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
separable_conv2d_26 (SeparableC (None, 128, 128, 64) 4736        activation_25[0][0]              
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 128, 128, 64) 256         separable_conv2d_26[0][0]        
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 128, 128, 64) 0           batch_normalization_26[0][0]     
__________________________________________________________________________________________________
conv2d_transpose_3 (Conv2DTrans (None, 256, 256, 32) 18464       activation_26[0][0]              
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 256, 256, 64) 0           conv2d_transpose_3[0][0]         
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
separable_conv2d_27 (SeparableC (None, 256, 256, 32) 2656        concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 256, 256, 32) 128         separable_conv2d_27[0][0]        
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 256, 256, 32) 0           batch_normalization_27[0][0]     
__________________________________________________________________________________________________
separable_conv2d_28 (SeparableC (None, 256, 256, 32) 1344        activation_27[0][0]              
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 256, 256, 32) 128         separable_conv2d_28[0][0]        
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 256, 256, 32) 0           batch_normalization_28[0][0]     
__________________________________________________________________________________________________
separable_conv2d_29 (SeparableC (None, 256, 256, 32) 1344        activation_28[0][0]              
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 256, 256, 32) 128         separable_conv2d_29[0][0]        
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 256, 256, 32) 0           batch_normalization_29[0][0]     
__________________________________________________________________________________________________
conv2d_transpose_4 (Conv2DTrans (None, 512, 512, 16) 4624        activation_29[0][0]              
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 512, 512, 32) 0           conv2d_transpose_4[0][0]         
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
separable_conv2d_30 (SeparableC (None, 512, 512, 16) 816         concatenate_4[0][0]              
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 512, 512, 16) 64          separable_conv2d_30[0][0]        
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 512, 512, 16) 0           batch_normalization_30[0][0]     
__________________________________________________________________________________________________
separable_conv2d_31 (SeparableC (None, 512, 512, 16) 416         activation_30[0][0]              
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 512, 512, 16) 64          separable_conv2d_31[0][0]        
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 512, 512, 16) 0           batch_normalization_31[0][0]     
__________________________________________________________________________________________________
separable_conv2d_32 (SeparableC (None, 512, 512, 16) 416         activation_31[0][0]              
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 512, 512, 16) 64          separable_conv2d_32[0][0]        
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 512, 512, 16) 0           batch_normalization_32[0][0]     
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 512, 512, 3)  51          activation_32[0][0]              
==================================================================================================
Total params: 2,542,110
Trainable params: 2,533,854
Non-trainable params: 8,256
__________________________________________________________________________________________________
None
2021-04-10 07:37:08.148256: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-04-10 07:37:08.172791: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2999955000 Hz
Epoch 1/500
36/36 [==============================] - 526s 14s/step - loss: 0.7241 - accuracy: 0.2737 - val_loss: 0.6874 - val_accuracy: 5.9319e-05

Epoch 00001: val_loss improved from inf to 0.68736, saving model to model-proba5-3.h5
Epoch 2/500
36/36 [==============================] - 538s 15s/step - loss: 0.6543 - accuracy: 0.0654 - val_loss: 0.6858 - val_accuracy: 0.9999

Epoch 00002: val_loss improved from 0.68736 to 0.68579, saving model to model-proba5-3.h5
Epoch 3/500
36/36 [==============================] - 504s 14s/step - loss: 0.6277 - accuracy: 0.1286 - val_loss: 0.6859 - val_accuracy: 0.9960

Epoch 00003: val_loss did not improve from 0.68579
Epoch 4/500
36/36 [==============================] - 525s 15s/step - loss: 0.6221 - accuracy: 0.2347 - val_loss: 0.6864 - val_accuracy: 0.9925

Epoch 00004: val_loss did not improve from 0.68579
Epoch 5/500
36/36 [==============================] - 522s 14s/step - loss: 0.6144 - accuracy: 0.1953 - val_loss: 0.6870 - val_accuracy: 8.9607e-04

Epoch 00005: val_loss did not improve from 0.68579
Epoch 6/500
36/36 [==============================] - 518s 14s/step - loss: 0.6125 - accuracy: 0.2264 - val_loss: 0.6872 - val_accuracy: 9.1467e-04

Epoch 00006: val_loss did not improve from 0.68579
Epoch 7/500
36/36 [==============================] - 515s 14s/step - loss: 0.6045 - accuracy: 0.1892 - val_loss: 0.6815 - val_accuracy: 0.0016

Epoch 00007: val_loss improved from 0.68579 to 0.68148, saving model to model-proba5-3.h5
Epoch 8/500
36/36 [==============================] - 533s 15s/step - loss: 0.6021 - accuracy: 0.2610 - val_loss: 0.6760 - val_accuracy: 0.0490

Epoch 00008: val_loss improved from 0.68148 to 0.67595, saving model to model-proba5-3.h5
Epoch 9/500
36/36 [==============================] - 518s 14s/step - loss: 0.5979 - accuracy: 0.2287 - val_loss: 0.6623 - val_accuracy: 0.3817

Epoch 00009: val_loss improved from 0.67595 to 0.66228, saving model to model-proba5-3.h5
Epoch 10/500
36/36 [==============================] - 530s 14s/step - loss: 0.5928 - accuracy: 0.2907 - val_loss: 0.6482 - val_accuracy: 0.7331

Epoch 00010: val_loss improved from 0.66228 to 0.64823, saving model to model-proba5-3.h5
Epoch 11/500
36/36 [==============================] - 526s 15s/step - loss: 0.5927 - accuracy: 0.2659 - val_loss: 0.6452 - val_accuracy: 0.4920

Epoch 00011: val_loss improved from 0.64823 to 0.64518, saving model to model-proba5-3.h5
Epoch 12/500
36/36 [==============================] - 540s 15s/step - loss: 0.5766 - accuracy: 0.3819 - val_loss: 0.6464 - val_accuracy: 0.2136

Epoch 00012: val_loss did not improve from 0.64518
Epoch 13/500
36/36 [==============================] - 516s 14s/step - loss: 0.5849 - accuracy: 0.2878 - val_loss: 0.6388 - val_accuracy: 0.4804

Epoch 00013: val_loss improved from 0.64518 to 0.63876, saving model to model-proba5-3.h5
Epoch 14/500
36/36 [==============================] - 522s 14s/step - loss: 0.5733 - accuracy: 0.2817 - val_loss: 0.6283 - val_accuracy: 0.4094

Epoch 00014: val_loss improved from 0.63876 to 0.62828, saving model to model-proba5-3.h5
Epoch 15/500
36/36 [==============================] - 523s 15s/step - loss: 0.5720 - accuracy: 0.3058 - val_loss: 0.6283 - val_accuracy: 0.4652

Epoch 00015: val_loss did not improve from 0.62828
Epoch 16/500
36/36 [==============================] - 502s 14s/step - loss: 0.5708 - accuracy: 0.3650 - val_loss: 0.7906 - val_accuracy: 0.2548

Epoch 00016: val_loss did not improve from 0.62828
Epoch 17/500
36/36 [==============================] - 537s 15s/step - loss: 0.5532 - accuracy: 0.4183 - val_loss: 0.6125 - val_accuracy: 0.4508

Epoch 00017: val_loss improved from 0.62828 to 0.61252, saving model to model-proba5-3.h5
Epoch 18/500
36/36 [==============================] - 519s 14s/step - loss: 0.5515 - accuracy: 0.3362 - val_loss: 0.6138 - val_accuracy: 0.2182

Epoch 00018: val_loss did not improve from 0.61252
Epoch 19/500
36/36 [==============================] - 550s 15s/step - loss: 0.5557 - accuracy: 0.3777 - val_loss: 0.6608 - val_accuracy: 0.4046

Epoch 00019: val_loss did not improve from 0.61252
Epoch 20/500
36/36 [==============================] - 547s 15s/step - loss: 0.5484 - accuracy: 0.3157 - val_loss: 0.6672 - val_accuracy: 0.2584

Epoch 00020: val_loss did not improve from 0.61252
Epoch 21/500
36/36 [==============================] - 553s 15s/step - loss: 0.5356 - accuracy: 0.4230 - val_loss: 0.6621 - val_accuracy: 0.4323

Epoch 00021: val_loss did not improve from 0.61252
Epoch 22/500
36/36 [==============================] - 542s 15s/step - loss: 0.5331 - accuracy: 0.4110 - val_loss: 0.5953 - val_accuracy: 0.3791

Epoch 00022: val_loss improved from 0.61252 to 0.59534, saving model to model-proba5-3.h5
Epoch 23/500
36/36 [==============================] - 538s 15s/step - loss: 0.5406 - accuracy: 0.3721 - val_loss: 0.6700 - val_accuracy: 0.3512

Epoch 00023: val_loss did not improve from 0.59534
Epoch 24/500
36/36 [==============================] - 531s 15s/step - loss: 0.5286 - accuracy: 0.3844 - val_loss: 0.6075 - val_accuracy: 0.4372

Epoch 00024: val_loss did not improve from 0.59534
Epoch 25/500
36/36 [==============================] - 544s 15s/step - loss: 0.5283 - accuracy: 0.3711 - val_loss: 0.5935 - val_accuracy: 0.5574

Epoch 00025: val_loss improved from 0.59534 to 0.59347, saving model to model-proba5-3.h5
Epoch 26/500
36/36 [==============================] - 521s 14s/step - loss: 0.5212 - accuracy: 0.4075 - val_loss: 0.6635 - val_accuracy: 0.5337

Epoch 00026: val_loss did not improve from 0.59347
Epoch 27/500
36/36 [==============================] - 515s 14s/step - loss: 0.5217 - accuracy: 0.3923 - val_loss: 0.6045 - val_accuracy: 0.4459

Epoch 00027: val_loss did not improve from 0.59347
Epoch 28/500
36/36 [==============================] - 545s 15s/step - loss: 0.5246 - accuracy: 0.3910 - val_loss: 0.5860 - val_accuracy: 0.5145

Epoch 00028: val_loss improved from 0.59347 to 0.58604, saving model to model-proba5-3.h5
Epoch 29/500
36/36 [==============================] - 553s 15s/step - loss: 0.5208 - accuracy: 0.3344 - val_loss: 0.7200 - val_accuracy: 0.4509

Epoch 00029: val_loss did not improve from 0.58604
Epoch 30/500
36/36 [==============================] - 503s 14s/step - loss: 0.5168 - accuracy: 0.3949 - val_loss: 0.6591 - val_accuracy: 0.4484

Epoch 00030: val_loss did not improve from 0.58604
Epoch 31/500
36/36 [==============================] - 534s 15s/step - loss: 0.5090 - accuracy: 0.3834 - val_loss: 0.6384 - val_accuracy: 0.4415

Epoch 00031: val_loss did not improve from 0.58604
Epoch 32/500
36/36 [==============================] - 554s 15s/step - loss: 0.5100 - accuracy: 0.3473 - val_loss: 0.7103 - val_accuracy: 0.3924

Epoch 00032: val_loss did not improve from 0.58604
Epoch 33/500
36/36 [==============================] - 550s 15s/step - loss: 0.5003 - accuracy: 0.3984 - val_loss: 0.6282 - val_accuracy: 0.4024

Epoch 00033: val_loss did not improve from 0.58604
Epoch 34/500
36/36 [==============================] - 540s 15s/step - loss: 0.5028 - accuracy: 0.3684 - val_loss: 0.6331 - val_accuracy: 0.4578

Epoch 00034: val_loss did not improve from 0.58604
Epoch 35/500
36/36 [==============================] - 549s 15s/step - loss: 0.5006 - accuracy: 0.3610 - val_loss: 0.6353 - val_accuracy: 0.3166

Epoch 00035: val_loss did not improve from 0.58604
Epoch 36/500
36/36 [==============================] - 530s 14s/step - loss: 0.5007 - accuracy: 0.3325 - val_loss: 0.6054 - val_accuracy: 0.3359

Epoch 00036: val_loss did not improve from 0.58604
Epoch 37/500
36/36 [==============================] - 536s 15s/step - loss: 0.4916 - accuracy: 0.4234 - val_loss: 0.6382 - val_accuracy: 0.4203

Epoch 00037: val_loss did not improve from 0.58604
Epoch 38/500
36/36 [==============================] - 558s 15s/step - loss: 0.4874 - accuracy: 0.4095 - val_loss: 0.6610 - val_accuracy: 0.4169

Epoch 00038: val_loss did not improve from 0.58604
Epoch 39/500
36/36 [==============================] - 552s 15s/step - loss: 0.4937 - accuracy: 0.3797 - val_loss: 0.6788 - val_accuracy: 0.4404

Epoch 00039: val_loss did not improve from 0.58604
Epoch 40/500
36/36 [==============================] - 533s 15s/step - loss: 0.4916 - accuracy: 0.4157 - val_loss: 0.6967 - val_accuracy: 0.4714

Epoch 00040: val_loss did not improve from 0.58604
Epoch 41/500
36/36 [==============================] - 516s 14s/step - loss: 0.4915 - accuracy: 0.3681 - val_loss: 0.6116 - val_accuracy: 0.4603

Epoch 00041: val_loss did not improve from 0.58604
Epoch 42/500
36/36 [==============================] - 543s 15s/step - loss: 0.4817 - accuracy: 0.4055 - val_loss: 0.5917 - val_accuracy: 0.5759

Epoch 00042: val_loss did not improve from 0.58604
Epoch 43/500
36/36 [==============================] - 504s 14s/step - loss: 0.4849 - accuracy: 0.3722 - val_loss: 0.6730 - val_accuracy: 0.3182

Epoch 00043: val_loss did not improve from 0.58604
Epoch 44/500
36/36 [==============================] - 527s 15s/step - loss: 0.4958 - accuracy: 0.3671 - val_loss: 0.6078 - val_accuracy: 0.4116

Epoch 00044: val_loss did not improve from 0.58604
Epoch 45/500
36/36 [==============================] - 509s 14s/step - loss: 0.4764 - accuracy: 0.3239 - val_loss: 0.6234 - val_accuracy: 0.4967

Epoch 00045: val_loss did not improve from 0.58604
Epoch 46/500
36/36 [==============================] - 511s 14s/step - loss: 0.4782 - accuracy: 0.3630 - val_loss: 0.6079 - val_accuracy: 0.5248

Epoch 00046: val_loss did not improve from 0.58604
Epoch 47/500
36/36 [==============================] - 512s 14s/step - loss: 0.4773 - accuracy: 0.3245 - val_loss: 0.6165 - val_accuracy: 0.5902

Epoch 00047: val_loss did not improve from 0.58604
Epoch 48/500
36/36 [==============================] - 539s 15s/step - loss: 0.4650 - accuracy: 0.3908 - val_loss: 0.6282 - val_accuracy: 0.4436

Epoch 00048: val_loss did not improve from 0.58604
Epoch 49/500
36/36 [==============================] - 549s 15s/step - loss: 0.4724 - accuracy: 0.3311 - val_loss: 0.5984 - val_accuracy: 0.5250

Epoch 00049: val_loss did not improve from 0.58604
Epoch 50/500
36/36 [==============================] - 549s 15s/step - loss: 0.4646 - accuracy: 0.3694 - val_loss: 0.7457 - val_accuracy: 0.4562

Epoch 00050: val_loss did not improve from 0.58604
Epoch 51/500
36/36 [==============================] - 529s 15s/step - loss: 0.4718 - accuracy: 0.2902 - val_loss: 0.7444 - val_accuracy: 0.4717

Epoch 00051: val_loss did not improve from 0.58604
Epoch 52/500
36/36 [==============================] - 523s 14s/step - loss: 0.4733 - accuracy: 0.3418 - val_loss: 0.6869 - val_accuracy: 0.3948

Epoch 00052: val_loss did not improve from 0.58604
Epoch 53/500
36/36 [==============================] - 513s 14s/step - loss: 0.4647 - accuracy: 0.3634 - val_loss: 0.6475 - val_accuracy: 0.4818

Epoch 00053: val_loss did not improve from 0.58604
Epoch 54/500
36/36 [==============================] - 532s 15s/step - loss: 0.4612 - accuracy: 0.3805 - val_loss: 0.7291 - val_accuracy: 0.4282

Epoch 00054: val_loss did not improve from 0.58604
Epoch 55/500
36/36 [==============================] - 539s 15s/step - loss: 0.4695 - accuracy: 0.3090 - val_loss: 0.6568 - val_accuracy: 0.5252

Epoch 00055: val_loss did not improve from 0.58604
Epoch 56/500
36/36 [==============================] - 538s 15s/step - loss: 0.4565 - accuracy: 0.4276 - val_loss: 0.6809 - val_accuracy: 0.4657

Epoch 00056: val_loss did not improve from 0.58604
Epoch 57/500
36/36 [==============================] - 531s 15s/step - loss: 0.4538 - accuracy: 0.3665 - val_loss: 0.7596 - val_accuracy: 0.2824

Epoch 00057: val_loss did not improve from 0.58604
Epoch 58/500
36/36 [==============================] - 542s 15s/step - loss: 0.4635 - accuracy: 0.2894 - val_loss: 0.6920 - val_accuracy: 0.3524

Epoch 00058: val_loss did not improve from 0.58604
Epoch 59/500
36/36 [==============================] - 534s 15s/step - loss: 0.4598 - accuracy: 0.3006 - val_loss: 0.6242 - val_accuracy: 0.3396

Epoch 00059: val_loss did not improve from 0.58604
Epoch 60/500
36/36 [==============================] - 524s 15s/step - loss: 0.4577 - accuracy: 0.3198 - val_loss: 0.6763 - val_accuracy: 0.3940

Epoch 00060: val_loss did not improve from 0.58604
Epoch 61/500
36/36 [==============================] - 522s 14s/step - loss: 0.4503 - accuracy: 0.3305 - val_loss: 0.7407 - val_accuracy: 0.3668

Epoch 00061: val_loss did not improve from 0.58604
Epoch 62/500
36/36 [==============================] - 519s 14s/step - loss: 0.4427 - accuracy: 0.3656 - val_loss: 0.6486 - val_accuracy: 0.5561

Epoch 00062: val_loss did not improve from 0.58604
Epoch 63/500
36/36 [==============================] - 496s 14s/step - loss: 0.4495 - accuracy: 0.3721 - val_loss: 0.7650 - val_accuracy: 0.4717

Epoch 00063: val_loss did not improve from 0.58604
Epoch 64/500
36/36 [==============================] - 541s 15s/step - loss: 0.4451 - accuracy: 0.3511 - val_loss: 0.6536 - val_accuracy: 0.5428

Epoch 00064: val_loss did not improve from 0.58604
Epoch 65/500
36/36 [==============================] - 508s 14s/step - loss: 0.4522 - accuracy: 0.2954 - val_loss: 0.6529 - val_accuracy: 0.5573

Epoch 00065: val_loss did not improve from 0.58604
Epoch 66/500
36/36 [==============================] - 514s 14s/step - loss: 0.4453 - accuracy: 0.3729 - val_loss: 0.6385 - val_accuracy: 0.3717

Epoch 00066: val_loss did not improve from 0.58604
Epoch 67/500
36/36 [==============================] - 508s 14s/step - loss: 0.4427 - accuracy: 0.3302 - val_loss: 0.6347 - val_accuracy: 0.4064

Epoch 00067: val_loss did not improve from 0.58604
Epoch 68/500
36/36 [==============================] - 534s 15s/step - loss: 0.4371 - accuracy: 0.4043 - val_loss: 0.6693 - val_accuracy: 0.4327

Epoch 00068: val_loss did not improve from 0.58604
Epoch 69/500
36/36 [==============================] - 529s 15s/step - loss: 0.4428 - accuracy: 0.3242 - val_loss: 0.6522 - val_accuracy: 0.4787

Epoch 00069: val_loss did not improve from 0.58604
Epoch 70/500
36/36 [==============================] - 541s 15s/step - loss: 0.4306 - accuracy: 0.3745 - val_loss: 0.6802 - val_accuracy: 0.5599

Epoch 00070: val_loss did not improve from 0.58604
Epoch 71/500
36/36 [==============================] - 519s 14s/step - loss: 0.4426 - accuracy: 0.3478 - val_loss: 0.7621 - val_accuracy: 0.5983

Epoch 00071: val_loss did not improve from 0.58604
Epoch 72/500
36/36 [==============================] - 530s 15s/step - loss: 0.4283 - accuracy: 0.3738 - val_loss: 0.6558 - val_accuracy: 0.6463

Epoch 00072: val_loss did not improve from 0.58604
Epoch 73/500
36/36 [==============================] - 530s 14s/step - loss: 0.4399 - accuracy: 0.3753 - val_loss: 0.6903 - val_accuracy: 0.4091

Epoch 00073: val_loss did not improve from 0.58604
Epoch 74/500
36/36 [==============================] - 547s 15s/step - loss: 0.4391 - accuracy: 0.3603 - val_loss: 0.6485 - val_accuracy: 0.5865

Epoch 00074: val_loss did not improve from 0.58604
Epoch 75/500
36/36 [==============================] - 527s 15s/step - loss: 0.4384 - accuracy: 0.2936 - val_loss: 0.6313 - val_accuracy: 0.5182

Epoch 00075: val_loss did not improve from 0.58604
Epoch 76/500
36/36 [==============================] - 545s 15s/step - loss: 0.4258 - accuracy: 0.3510 - val_loss: 0.6630 - val_accuracy: 0.5386

Epoch 00076: val_loss did not improve from 0.58604
Epoch 77/500
36/36 [==============================] - 523s 15s/step - loss: 0.4331 - accuracy: 0.3307 - val_loss: 0.6668 - val_accuracy: 0.4302

Epoch 00077: val_loss did not improve from 0.58604
Epoch 78/500
36/36 [==============================] - 517s 14s/step - loss: 0.4250 - accuracy: 0.3884 - val_loss: 0.7398 - val_accuracy: 0.3395

Epoch 00078: val_loss did not improve from 0.58604
Epoch 79/500
36/36 [==============================] - 525s 15s/step - loss: 0.4244 - accuracy: 0.4031 - val_loss: 0.6633 - val_accuracy: 0.3116

Epoch 00079: val_loss did not improve from 0.58604
Epoch 80/500
36/36 [==============================] - 563s 16s/step - loss: 0.4359 - accuracy: 0.3150 - val_loss: 0.7061 - val_accuracy: 0.2617

Epoch 00080: val_loss did not improve from 0.58604
Epoch 81/500
36/36 [==============================] - 566s 16s/step - loss: 0.4284 - accuracy: 0.3375 - val_loss: 0.6616 - val_accuracy: 0.3032

Epoch 00081: val_loss did not improve from 0.58604
Epoch 82/500
36/36 [==============================] - 533s 15s/step - loss: 0.4311 - accuracy: 0.3914 - val_loss: 0.6760 - val_accuracy: 0.3111

Epoch 00082: val_loss did not improve from 0.58604
Epoch 83/500
36/36 [==============================] - 538s 15s/step - loss: 0.4207 - accuracy: 0.3539 - val_loss: 0.6709 - val_accuracy: 0.5491

Epoch 00083: val_loss did not improve from 0.58604
Epoch 84/500
36/36 [==============================] - 520s 14s/step - loss: 0.4306 - accuracy: 0.3540 - val_loss: 0.6661 - val_accuracy: 0.4890

Epoch 00084: val_loss did not improve from 0.58604
Epoch 85/500
36/36 [==============================] - 555s 15s/step - loss: 0.4235 - accuracy: 0.3222 - val_loss: 0.6427 - val_accuracy: 0.3324

Epoch 00085: val_loss did not improve from 0.58604
Epoch 86/500
36/36 [==============================] - 549s 15s/step - loss: 0.4252 - accuracy: 0.3893 - val_loss: 0.6573 - val_accuracy: 0.2348

Epoch 00086: val_loss did not improve from 0.58604
Epoch 87/500
36/36 [==============================] - 507s 14s/step - loss: 0.4190 - accuracy: 0.3281 - val_loss: 0.6767 - val_accuracy: 0.2499

Epoch 00087: val_loss did not improve from 0.58604
Epoch 88/500
36/36 [==============================] - 542s 15s/step - loss: 0.4276 - accuracy: 0.2910 - val_loss: 0.6594 - val_accuracy: 0.3283

Epoch 00088: val_loss did not improve from 0.58604
Epoch 89/500
36/36 [==============================] - 535s 15s/step - loss: 0.4275 - accuracy: 0.3125 - val_loss: 0.6417 - val_accuracy: 0.2601

Epoch 00089: val_loss did not improve from 0.58604
Epoch 90/500
36/36 [==============================] - 530s 15s/step - loss: 0.4242 - accuracy: 0.3396 - val_loss: 0.6490 - val_accuracy: 0.2685

Epoch 00090: val_loss did not improve from 0.58604
Epoch 91/500
36/36 [==============================] - 542s 15s/step - loss: 0.4227 - accuracy: 0.3068 - val_loss: 0.6720 - val_accuracy: 0.3204

Epoch 00091: val_loss did not improve from 0.58604
Epoch 92/500
36/36 [==============================] - 547s 15s/step - loss: 0.4373 - accuracy: 0.2799 - val_loss: 0.6544 - val_accuracy: 0.3070

Epoch 00092: val_loss did not improve from 0.58604
Epoch 93/500
36/36 [==============================] - 512s 14s/step - loss: 0.4249 - accuracy: 0.3230 - val_loss: 0.7061 - val_accuracy: 0.2972

Epoch 00093: val_loss did not improve from 0.58604
Epoch 94/500
36/36 [==============================] - 514s 14s/step - loss: 0.4220 - accuracy: 0.3065 - val_loss: 0.6881 - val_accuracy: 0.4453

Epoch 00094: val_loss did not improve from 0.58604
Epoch 95/500
36/36 [==============================] - 566s 16s/step - loss: 0.4239 - accuracy: 0.2901 - val_loss: 0.6553 - val_accuracy: 0.3434

Epoch 00095: val_loss did not improve from 0.58604
Epoch 96/500
36/36 [==============================] - 522s 14s/step - loss: 0.4313 - accuracy: 0.3430 - val_loss: 0.7334 - val_accuracy: 0.2742

Epoch 00096: val_loss did not improve from 0.58604
Epoch 97/500
36/36 [==============================] - 530s 15s/step - loss: 0.4189 - accuracy: 0.2931 - val_loss: 0.6730 - val_accuracy: 0.3295

Epoch 00097: val_loss did not improve from 0.58604
Epoch 98/500
36/36 [==============================] - 519s 15s/step - loss: 0.4145 - accuracy: 0.3939 - val_loss: 0.6517 - val_accuracy: 0.3218

Epoch 00098: val_loss did not improve from 0.58604
Epoch 99/500
36/36 [==============================] - 527s 15s/step - loss: 0.4213 - accuracy: 0.3160 - val_loss: 0.6583 - val_accuracy: 0.3154

Epoch 00099: val_loss did not improve from 0.58604
Epoch 100/500
36/36 [==============================] - 506s 14s/step - loss: 0.4254 - accuracy: 0.2723 - val_loss: 0.6444 - val_accuracy: 0.3605

Epoch 00100: val_loss did not improve from 0.58604
Epoch 101/500
36/36 [==============================] - 536s 15s/step - loss: 0.4183 - accuracy: 0.3453 - val_loss: 0.6547 - val_accuracy: 0.2827

Epoch 00101: val_loss did not improve from 0.58604
Epoch 102/500
36/36 [==============================] - 550s 15s/step - loss: 0.4188 - accuracy: 0.3143 - val_loss: 0.6691 - val_accuracy: 0.3075

Epoch 00102: val_loss did not improve from 0.58604
Epoch 103/500
36/36 [==============================] - 526s 15s/step - loss: 0.4153 - accuracy: 0.3469 - val_loss: 0.6407 - val_accuracy: 0.2757

Epoch 00103: val_loss did not improve from 0.58604
Epoch 104/500
36/36 [==============================] - 533s 15s/step - loss: 0.4143 - accuracy: 0.3332 - val_loss: 0.6765 - val_accuracy: 0.3384

Epoch 00104: val_loss did not improve from 0.58604
Epoch 105/500
36/36 [==============================] - 537s 15s/step - loss: 0.4149 - accuracy: 0.3895 - val_loss: 0.6563 - val_accuracy: 0.2807

Epoch 00105: val_loss did not improve from 0.58604
Epoch 106/500
36/36 [==============================] - 562s 15s/step - loss: 0.4191 - accuracy: 0.2600 - val_loss: 0.6767 - val_accuracy: 0.3266

Epoch 00106: val_loss did not improve from 0.58604
Epoch 107/500
36/36 [==============================] - 539s 15s/step - loss: 0.4159 - accuracy: 0.3305 - val_loss: 0.6797 - val_accuracy: 0.2934

Epoch 00107: val_loss did not improve from 0.58604
Epoch 108/500
36/36 [==============================] - 564s 16s/step - loss: 0.4163 - accuracy: 0.3641 - val_loss: 0.6926 - val_accuracy: 0.2431

Epoch 00108: val_loss did not improve from 0.58604
Epoch 109/500
36/36 [==============================] - 520s 14s/step - loss: 0.4185 - accuracy: 0.3020 - val_loss: 0.7058 - val_accuracy: 0.2410

Epoch 00109: val_loss did not improve from 0.58604
Epoch 110/500
 7/36 [====>.........................] - ETA: 6:58 - loss: 0.3951 - accuracy: 0.4239packet_write_wait: Connection to 161.53.171.105 port 10003: Broken pipe
(base) Veronikas-MacBook-Pro:~ veronika$ 
